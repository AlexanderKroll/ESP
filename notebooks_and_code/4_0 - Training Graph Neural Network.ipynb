{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "C:\\Users\\alexk\\projects\\SubFinder\\notebooks_and_code\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "sys.path.append('.\\\\additional_code')\n",
    "from xgboost_training import *\n",
    "\n",
    "CURRENT_DIR = os.getcwd()\n",
    "print(CURRENT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and preprocessing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(join(CURRENT_DIR, \"..\" ,\"data\",\"splits\", \"df_train_with_ESM1b_ts.pkl\"))\n",
    "df_test = pd.read_pickle(join(CURRENT_DIR, \"..\" ,\"data\", \"splits\", \"df_test_with_ESM1b_ts.pkl\"))\n",
    "\n",
    "df_train = df_train.loc[df_train[\"evidence\"] == \"exp\"]\n",
    "df_test = df_test.loc[df_test[\"evidence\"] == \"exp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Balancing datasets such that we have same amount of negative as positive data samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_UIDs = df_train[\"Uniprot ID\"].loc[df_train[\"Binding\"] == 1]\n",
    "for UID in pos_UIDs:\n",
    "    n_pos = len(df_train.loc[df_train[\"Binding\"] == 1].loc[df_train[\"Uniprot ID\"] == UID])\n",
    "    help_df = df_train.loc[df_train[\"Binding\"] == 0].loc[df_train[\"Uniprot ID\"] == UID]\n",
    "    df_train.drop(list(help_df.index)[n_pos:], inplace= True)\n",
    "    df_train.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_UIDs = df_test[\"Uniprot ID\"].loc[df_test[\"Binding\"] == 1]\n",
    "for UID in pos_UIDs:\n",
    "    n_pos = len(df_test.loc[df_test[\"Binding\"] == 1].loc[df_test[\"Uniprot ID\"] == UID])\n",
    "    help_df = df_test.loc[df_test[\"Binding\"] == 0].loc[df_test[\"Uniprot ID\"] == UID]\n",
    "    df_test.drop(list(help_df.index)[n_pos:], inplace= True)\n",
    "    df_test.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Create dictionary with all target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_files = list(set(df_train[\"molecule ID\"])) + list(set(df_test[\"molecule ID\"]))\n",
    "mol_files = list(set(mol_files))\n",
    "\n",
    "target_variable_dict = {}\n",
    "target_variable_dict = create_target_dict(df = df_train, target_variable_dict = target_variable_dict)\n",
    "target_variable_dict = create_target_dict(df = df_test, target_variable_dict = target_variable_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Get list with input combinations of Uniprot ID and metabolite ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5003414600833163 0.5005673758865248\n",
      "29286 7050\n"
     ]
    }
   ],
   "source": [
    "train_IDs = get_uid_cid_IDs(df_train)\n",
    "test_IDs = get_uid_cid_IDs(df_test)\n",
    "\n",
    "print(np.mean(df_train[\"Binding\"]), np.mean(df_test[\"Binding\"]))\n",
    "print(len(train_IDs), len(test_IDs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculating input matrices for metabolites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Creating input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate_atom_and_bond_feature_vectors(mol_files = mol_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for mol_ID in mol_files:\n",
    "#    calculate_and_save_input_matrixes(molecule_ID = mol_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  (b) Removing all datapoints without molecule input file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"molecule ID\"] = [ID.replace(\":\", \"_\") for ID in df_train[\"substrate ID\"]]\n",
    "df_test[\"molecule ID\"] = [ID.replace(\":\", \"_\") for ID in df_test[\"substrate ID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Uniprot ID</th>\n",
       "      <th>molecule ID</th>\n",
       "      <th>evidence</th>\n",
       "      <th>Binding</th>\n",
       "      <th>type</th>\n",
       "      <th>substrate ID</th>\n",
       "      <th>ECFP</th>\n",
       "      <th>ESM1b</th>\n",
       "      <th>ESM1b_ts</th>\n",
       "      <th>GNN rep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q5B2F7</td>\n",
       "      <td>CHEBI_57344</td>\n",
       "      <td>exp</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHEBI:57344</td>\n",
       "      <td>0100000001000000000000000000000001000000000000...</td>\n",
       "      <td>[0.09207666, 0.18022089, 0.1191696, -0.0068351...</td>\n",
       "      <td>[-0.52362674, 0.5027057, -0.40282017, 0.742947...</td>\n",
       "      <td>[1577.9962, 10.317345, 29.326752, 233.01369, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q9SAH9</td>\n",
       "      <td>CHEBI_58349</td>\n",
       "      <td>exp</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHEBI:58349</td>\n",
       "      <td>0000000001000000100000100000000000000000000000...</td>\n",
       "      <td>[0.022810845, 0.1272514, -0.051154055, -0.0810...</td>\n",
       "      <td>[0.61918294, 0.121414125, 0.40603346, 1.126637...</td>\n",
       "      <td>[2261.6094, 0.0, 0.0, 115.09651, 179.84134, 46...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q8IPJ6</td>\n",
       "      <td>CHEBI_57776</td>\n",
       "      <td>exp</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHEBI:57776</td>\n",
       "      <td>0000000000000000000000000000010001000000000000...</td>\n",
       "      <td>[0.09814875, 0.22172487, 0.11138555, 0.0365497...</td>\n",
       "      <td>[0.29864457, 0.22536643, 0.27347004, -0.128196...</td>\n",
       "      <td>[791.13226, 7.796671, 0.0, 0.0, 4.66982, 10.69...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A1D5PCZ1</td>\n",
       "      <td>C00002</td>\n",
       "      <td>exp</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C00002</td>\n",
       "      <td>0000000001000000000000000000000000000000000000...</td>\n",
       "      <td>[-0.21187752, 0.08564956, 0.055316914, -0.0550...</td>\n",
       "      <td>[-0.86605054, -0.38922024, -0.539311, 1.373580...</td>\n",
       "      <td>[1238.0188, 0.0, 0.0, 42.365837, 74.54658, 28....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O22765</td>\n",
       "      <td>CHEBI_33384</td>\n",
       "      <td>exp</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHEBI:33384</td>\n",
       "      <td>0100000000000000000000000000000000000000000000...</td>\n",
       "      <td>[0.027133903, 0.33383188, -0.0057643764, -0.00...</td>\n",
       "      <td>[1.1005167, -1.0289398, -0.061415985, 0.988528...</td>\n",
       "      <td>[72.62339, 18.489643, 0.0, 50.355515, 13.49715...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29355</th>\n",
       "      <td>O54937</td>\n",
       "      <td>CHEBI_16199</td>\n",
       "      <td>exp</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHEBI:16199</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>[0.050787933, 0.20482497, -0.0821579, -0.03619...</td>\n",
       "      <td>[-0.6821743, -0.25235456, -0.06566423, 0.84851...</td>\n",
       "      <td>[10.734227, 0.0, 4.6557817, 1.7201436, 0.0, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29356</th>\n",
       "      <td>P42980</td>\n",
       "      <td>CHEBI_43474</td>\n",
       "      <td>exp</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHEBI:43474</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "      <td>[0.03450865, 0.10044937, -0.081294104, 0.03105...</td>\n",
       "      <td>[0.7604322, -0.6746883, 0.038595006, 0.1019296...</td>\n",
       "      <td>[32.170155, 0.0, 0.0, 0.0, 0.7738515, 47.04823...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29357</th>\n",
       "      <td>P31254</td>\n",
       "      <td>CHEBI_30616</td>\n",
       "      <td>exp</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHEBI:30616</td>\n",
       "      <td>0000000001000000000000000000000000000000000100...</td>\n",
       "      <td>[-0.10911206, 0.12464452, -0.006680568, 0.1137...</td>\n",
       "      <td>[0.5049492, 0.23488945, -0.7357721, 0.21344757...</td>\n",
       "      <td>[1288.9618, 0.0, 0.0, 76.52397, 73.09448, 37.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29358</th>\n",
       "      <td>C0HLL2</td>\n",
       "      <td>CHEBI_30616</td>\n",
       "      <td>exp</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHEBI:30616</td>\n",
       "      <td>0000000001000000000000000000000000000000000100...</td>\n",
       "      <td>[0.087619156, 0.30014926, 0.051759467, 0.07981...</td>\n",
       "      <td>[1.0081663, -0.47126764, 0.106960185, -0.28055...</td>\n",
       "      <td>[1288.9618, 0.0, 0.0, 76.52397, 73.09448, 37.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29359</th>\n",
       "      <td>Q8RVK9</td>\n",
       "      <td>C00002</td>\n",
       "      <td>exp</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C00002</td>\n",
       "      <td>0000000001000000000000000000000000000000000000...</td>\n",
       "      <td>[-0.0870039, 0.34124222, 0.20787948, -0.154150...</td>\n",
       "      <td>[-0.59816426, 0.41644707, -0.7390129, 1.428337...</td>\n",
       "      <td>[1238.0188, 0.0, 0.0, 42.365837, 74.54658, 28....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29286 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Uniprot ID  molecule ID evidence  Binding  type substrate ID  \\\n",
       "0          Q5B2F7  CHEBI_57344      exp        1   NaN  CHEBI:57344   \n",
       "1          Q9SAH9  CHEBI_58349      exp        1   NaN  CHEBI:58349   \n",
       "2          Q8IPJ6  CHEBI_57776      exp        1   NaN  CHEBI:57776   \n",
       "3      A0A1D5PCZ1       C00002      exp        1   NaN       C00002   \n",
       "4          O22765  CHEBI_33384      exp        1   NaN  CHEBI:33384   \n",
       "...           ...          ...      ...      ...   ...          ...   \n",
       "29355      O54937  CHEBI_16199      exp        0   NaN  CHEBI:16199   \n",
       "29356      P42980  CHEBI_43474      exp        0   NaN  CHEBI:43474   \n",
       "29357      P31254  CHEBI_30616      exp        0   NaN  CHEBI:30616   \n",
       "29358      C0HLL2  CHEBI_30616      exp        0   NaN  CHEBI:30616   \n",
       "29359      Q8RVK9       C00002      exp        0   NaN       C00002   \n",
       "\n",
       "                                                    ECFP  \\\n",
       "0      0100000001000000000000000000000001000000000000...   \n",
       "1      0000000001000000100000100000000000000000000000...   \n",
       "2      0000000000000000000000000000010001000000000000...   \n",
       "3      0000000001000000000000000000000000000000000000...   \n",
       "4      0100000000000000000000000000000000000000000000...   \n",
       "...                                                  ...   \n",
       "29355  0000000000000000000000000000000000000000000000...   \n",
       "29356  0000000000000000000000000000000000000000000000...   \n",
       "29357  0000000001000000000000000000000000000000000100...   \n",
       "29358  0000000001000000000000000000000000000000000100...   \n",
       "29359  0000000001000000000000000000000000000000000000...   \n",
       "\n",
       "                                                   ESM1b  \\\n",
       "0      [0.09207666, 0.18022089, 0.1191696, -0.0068351...   \n",
       "1      [0.022810845, 0.1272514, -0.051154055, -0.0810...   \n",
       "2      [0.09814875, 0.22172487, 0.11138555, 0.0365497...   \n",
       "3      [-0.21187752, 0.08564956, 0.055316914, -0.0550...   \n",
       "4      [0.027133903, 0.33383188, -0.0057643764, -0.00...   \n",
       "...                                                  ...   \n",
       "29355  [0.050787933, 0.20482497, -0.0821579, -0.03619...   \n",
       "29356  [0.03450865, 0.10044937, -0.081294104, 0.03105...   \n",
       "29357  [-0.10911206, 0.12464452, -0.006680568, 0.1137...   \n",
       "29358  [0.087619156, 0.30014926, 0.051759467, 0.07981...   \n",
       "29359  [-0.0870039, 0.34124222, 0.20787948, -0.154150...   \n",
       "\n",
       "                                                ESM1b_ts  \\\n",
       "0      [-0.52362674, 0.5027057, -0.40282017, 0.742947...   \n",
       "1      [0.61918294, 0.121414125, 0.40603346, 1.126637...   \n",
       "2      [0.29864457, 0.22536643, 0.27347004, -0.128196...   \n",
       "3      [-0.86605054, -0.38922024, -0.539311, 1.373580...   \n",
       "4      [1.1005167, -1.0289398, -0.061415985, 0.988528...   \n",
       "...                                                  ...   \n",
       "29355  [-0.6821743, -0.25235456, -0.06566423, 0.84851...   \n",
       "29356  [0.7604322, -0.6746883, 0.038595006, 0.1019296...   \n",
       "29357  [0.5049492, 0.23488945, -0.7357721, 0.21344757...   \n",
       "29358  [1.0081663, -0.47126764, 0.106960185, -0.28055...   \n",
       "29359  [-0.59816426, 0.41644707, -0.7390129, 1.428337...   \n",
       "\n",
       "                                                 GNN rep  \n",
       "0      [1577.9962, 10.317345, 29.326752, 233.01369, 4...  \n",
       "1      [2261.6094, 0.0, 0.0, 115.09651, 179.84134, 46...  \n",
       "2      [791.13226, 7.796671, 0.0, 0.0, 4.66982, 10.69...  \n",
       "3      [1238.0188, 0.0, 0.0, 42.365837, 74.54658, 28....  \n",
       "4      [72.62339, 18.489643, 0.0, 50.355515, 13.49715...  \n",
       "...                                                  ...  \n",
       "29355  [10.734227, 0.0, 4.6557817, 1.7201436, 0.0, 0....  \n",
       "29356  [32.170155, 0.0, 0.0, 0.0, 0.7738515, 47.04823...  \n",
       "29357  [1288.9618, 0.0, 0.0, 76.52397, 73.09448, 37.6...  \n",
       "29358  [1288.9618, 0.0, 0.0, 76.52397, 73.09448, 37.6...  \n",
       "29359  [1238.0188, 0.0, 0.0, 42.365837, 74.54658, 28....  \n",
       "\n",
       "[29286 rows x 10 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_mols = os.listdir(join(CURRENT_DIR, \"..\" ,\"data\", \"substrate_data\", \"GNN_input_matrices\"))\n",
    "valid_mols = [mol.split(\"_A\")[0] for mol in valid_mols]\n",
    "\n",
    "df_train = df_train.loc[df_train[\"molecule ID\"].isin(valid_mols)]\n",
    "df_test = df_test.loc[df_test[\"molecule ID\"].isin(valid_mols)]\n",
    "\n",
    "train_IDs = get_uid_cid_IDs(df_train)\n",
    "test_IDs = get_uid_cid_IDs(df_test)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Creating representations for the enzymes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids_list = list(set(df_train[\"Uniprot ID\"])) + list(set(df_test[\"Uniprot ID\"]))\n",
    "uids_list = list(set(uids_list))\n",
    "uid_to_emb = {}\n",
    "embeddings = np.zeros((0,1280))\n",
    "for uid in uids_list:\n",
    "    try:\n",
    "        emb = np.reshape(np.array(list(df_train[\"ESM1b\"].loc[df_train[\"Uniprot ID\"] == uid])[0]), (1,1280))\n",
    "    except IndexError:\n",
    "        try:\n",
    "            emb = np.reshape(np.array(list(df_test[\"ESM1b\"].loc[df_test[\"Uniprot ID\"] == uid])[0]), (1,1280))\n",
    "        except IndexError:\n",
    "            emb = np.reshape(np.array(list(df_validation[\"ESM1b\"].loc[df_validation[\"Uniprot ID\"] == uid])[0]), (1,1280))\n",
    "    embeddings = np.concatenate([embeddings, emb])\n",
    "    uid_to_emb[uid] = emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a PCA an the enzyme representations to get 50-dimensional representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "dim = 50\n",
    "\n",
    "pca = PCA(n_components = dim)\n",
    "pca.fit(embeddings)\n",
    "emb_pca = pca.transform(embeddings)\n",
    "\n",
    "#Calculate mean and std to normalize the PCA-transformed vectors\n",
    "mean = np.mean(emb_pca, axis = 0)\n",
    "std = np.std(emb_pca, axis = 0)\n",
    "\n",
    "uid_to_pca_emb = {}\n",
    "\n",
    "for i, uid in enumerate(uids_list):\n",
    "    uid_to_pca_emb[uid] = (emb_pca[i] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_to_emb = uid_to_pca_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training GNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  (a) Defining a DataGenerator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, split_IDs, folder):\n",
    "        self.all_IDs = split_IDs\n",
    "        self.folder = folder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_IDs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ID = self.all_IDs[idx]\n",
    "        try:\n",
    "            [uid,cid1, cid2] = ID.split(\"_\") \n",
    "            cid = cid1 +\"_\"+cid2\n",
    "        except ValueError:\n",
    "            [uid,cid] = ID.split(\"_\")\n",
    "            \n",
    "        XE = torch.tensor(np.load(join(self.folder, cid + '_XE.npy')), dtype = torch.float32)\n",
    "        X = torch.tensor(np.load(join(self.folder, cid + '_X.npy')), dtype = torch.float32)\n",
    "        A = torch.tensor(np.load(join(self.folder, cid + '_A.npy')), dtype = torch.float32)\n",
    "        ESM1b = torch.tensor(uid_to_emb[uid], dtype = torch.float32)\n",
    "        label = torch.tensor(target_variable_dict[ID], dtype= torch.float32)\n",
    "        return XE,X,A,ESM1b, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Splitting the training set in a validation and a training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(train_IDs) \n",
    "random.seed(1)\n",
    "random.shuffle(train_IDs)\n",
    "test_IDs = train_IDs[int(0.8*n):]\n",
    "train_IDs = train_IDs[:int(0.8*n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataset = CustomDataSet(folder  = join(CURRENT_DIR, \"..\" ,\"data\", \"substrate_data\",\n",
    "                                             \"GNN_input_matrices\"), split_IDs = train_IDs)\n",
    "train_loader = DataLoader(train_dataset , batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "test_dataset = CustomDataSet(folder  = join(CURRENT_DIR, \"..\" ,\"data\", \"substrate_data\",\n",
    "                                            \"GNN_input_matrices\"), split_IDs = test_IDs)\n",
    "test_loader = DataLoader(test_dataset , batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_batches = int(len(train_dataset)/batch_size)\n",
    "n_test_batches = int(len(test_dataset)/batch_size)\n",
    "train_batches = list(range(n_train_batches))\n",
    "test_batches = list(range(n_test_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Training GNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 0.699\n",
      "[1,    40] loss: 0.693\n",
      "[1,    60] loss: 0.692\n",
      "[1,    80] loss: 0.683\n",
      "[1,   100] loss: 0.691\n",
      "[1,   120] loss: 0.693\n",
      "[1,   140] loss: 0.689\n",
      "[1,   160] loss: 0.681\n",
      "[1,   180] loss: 0.691\n",
      "[1,   200] loss: 0.686\n",
      "[1,   220] loss: 0.682\n",
      "[1,   240] loss: 0.683\n",
      "[1,   260] loss: 0.675\n",
      "[1,   280] loss: 0.680\n",
      "[1,   300] loss: 0.684\n",
      "[1,   320] loss: 0.676\n",
      "[1,   340] loss: 0.686\n",
      "[1,   360] loss: 0.677\n",
      "Epoch: 0, Val. loss: 0.67, Val. acc: 0.57\n",
      "[2,    20] loss: 0.669\n",
      "[2,    40] loss: 0.669\n",
      "[2,    60] loss: 0.665\n",
      "[2,    80] loss: 0.667\n",
      "[2,   100] loss: 0.660\n",
      "[2,   120] loss: 0.663\n",
      "[2,   140] loss: 0.658\n",
      "[2,   160] loss: 0.662\n",
      "[2,   180] loss: 0.656\n",
      "[2,   200] loss: 0.655\n",
      "[2,   220] loss: 0.659\n",
      "[2,   240] loss: 0.654\n",
      "[2,   260] loss: 0.642\n",
      "[2,   280] loss: 0.648\n",
      "[2,   300] loss: 0.645\n",
      "[2,   320] loss: 0.657\n",
      "[2,   340] loss: 0.642\n",
      "[2,   360] loss: 0.645\n",
      "Epoch: 1, Val. loss: 0.65, Val. acc: 0.62\n",
      "[3,    20] loss: 0.647\n",
      "[3,    40] loss: 0.634\n",
      "[3,    60] loss: 0.619\n",
      "[3,    80] loss: 0.631\n",
      "[3,   100] loss: 0.626\n",
      "[3,   120] loss: 0.616\n",
      "[3,   140] loss: 0.628\n",
      "[3,   160] loss: 0.623\n",
      "[3,   180] loss: 0.617\n",
      "[3,   200] loss: 0.620\n",
      "[3,   220] loss: 0.608\n",
      "[3,   240] loss: 0.613\n",
      "[3,   260] loss: 0.625\n",
      "[3,   280] loss: 0.631\n",
      "[3,   300] loss: 0.612\n",
      "[3,   320] loss: 0.604\n",
      "[3,   340] loss: 0.614\n",
      "[3,   360] loss: 0.607\n",
      "Epoch: 2, Val. loss: 0.62, Val. acc: 0.66\n",
      "[4,    20] loss: 0.591\n",
      "[4,    40] loss: 0.594\n",
      "[4,    60] loss: 0.590\n",
      "[4,    80] loss: 0.594\n",
      "[4,   100] loss: 0.605\n",
      "[4,   120] loss: 0.597\n",
      "[4,   140] loss: 0.588\n",
      "[4,   160] loss: 0.604\n",
      "[4,   180] loss: 0.593\n",
      "[4,   200] loss: 0.593\n",
      "[4,   220] loss: 0.597\n",
      "[4,   240] loss: 0.587\n",
      "[4,   260] loss: 0.583\n",
      "[4,   280] loss: 0.601\n",
      "[4,   300] loss: 0.604\n",
      "[4,   320] loss: 0.602\n",
      "[4,   340] loss: 0.583\n",
      "[4,   360] loss: 0.579\n",
      "Epoch: 3, Val. loss: 0.6, Val. acc: 0.67\n",
      "[5,    20] loss: 0.578\n",
      "[5,    40] loss: 0.591\n",
      "[5,    60] loss: 0.589\n",
      "[5,    80] loss: 0.589\n",
      "[5,   100] loss: 0.582\n",
      "[5,   120] loss: 0.562\n",
      "[5,   140] loss: 0.589\n",
      "[5,   160] loss: 0.570\n",
      "[5,   180] loss: 0.591\n",
      "[5,   200] loss: 0.574\n",
      "[5,   220] loss: 0.557\n",
      "[5,   240] loss: 0.585\n",
      "[5,   260] loss: 0.571\n",
      "[5,   280] loss: 0.582\n",
      "[5,   300] loss: 0.576\n",
      "[5,   320] loss: 0.576\n",
      "[5,   340] loss: 0.572\n",
      "[5,   360] loss: 0.565\n",
      "Epoch: 4, Val. loss: 0.6, Val. acc: 0.66\n",
      "[6,    20] loss: 0.558\n",
      "[6,    40] loss: 0.549\n",
      "[6,    60] loss: 0.574\n",
      "[6,    80] loss: 0.576\n",
      "[6,   100] loss: 0.571\n",
      "[6,   120] loss: 0.575\n",
      "[6,   140] loss: 0.560\n",
      "[6,   160] loss: 0.571\n",
      "[6,   180] loss: 0.547\n",
      "[6,   200] loss: 0.563\n",
      "[6,   220] loss: 0.551\n",
      "[6,   240] loss: 0.551\n",
      "[6,   260] loss: 0.571\n",
      "[6,   280] loss: 0.573\n",
      "[6,   300] loss: 0.574\n",
      "[6,   320] loss: 0.567\n",
      "[6,   340] loss: 0.569\n",
      "[6,   360] loss: 0.591\n",
      "Epoch: 5, Val. loss: 0.57, Val. acc: 0.69\n",
      "[7,    20] loss: 0.554\n",
      "[7,    40] loss: 0.542\n",
      "[7,    60] loss: 0.551\n",
      "[7,    80] loss: 0.553\n",
      "[7,   100] loss: 0.555\n",
      "[7,   120] loss: 0.549\n",
      "[7,   140] loss: 0.544\n",
      "[7,   160] loss: 0.569\n",
      "[7,   180] loss: 0.557\n",
      "[7,   200] loss: 0.574\n",
      "[7,   220] loss: 0.552\n",
      "[7,   240] loss: 0.549\n",
      "[7,   260] loss: 0.527\n",
      "[7,   280] loss: 0.545\n",
      "[7,   300] loss: 0.569\n",
      "[7,   320] loss: 0.555\n",
      "[7,   340] loss: 0.559\n",
      "[7,   360] loss: 0.571\n",
      "Epoch: 6, Val. loss: 0.55, Val. acc: 0.68\n",
      "[8,    20] loss: 0.541\n",
      "[8,    40] loss: 0.562\n",
      "[8,    60] loss: 0.540\n",
      "[8,    80] loss: 0.536\n",
      "[8,   100] loss: 0.537\n",
      "[8,   120] loss: 0.527\n",
      "[8,   140] loss: 0.547\n",
      "[8,   160] loss: 0.551\n",
      "[8,   180] loss: 0.557\n",
      "[8,   200] loss: 0.525\n",
      "[8,   220] loss: 0.539\n",
      "[8,   240] loss: 0.561\n",
      "[8,   260] loss: 0.563\n",
      "[8,   280] loss: 0.546\n",
      "[8,   300] loss: 0.549\n",
      "[8,   320] loss: 0.531\n",
      "[8,   340] loss: 0.537\n",
      "[8,   360] loss: 0.543\n",
      "Epoch: 7, Val. loss: 0.56, Val. acc: 0.7\n",
      "[9,    20] loss: 0.532\n",
      "[9,    40] loss: 0.548\n",
      "[9,    60] loss: 0.533\n",
      "[9,    80] loss: 0.536\n",
      "[9,   100] loss: 0.517\n",
      "[9,   120] loss: 0.555\n",
      "[9,   140] loss: 0.512\n",
      "[9,   160] loss: 0.523\n",
      "[9,   180] loss: 0.525\n",
      "[9,   200] loss: 0.529\n",
      "[9,   220] loss: 0.560\n",
      "[9,   240] loss: 0.528\n",
      "[9,   260] loss: 0.527\n",
      "[9,   280] loss: 0.551\n",
      "[9,   300] loss: 0.550\n",
      "[9,   320] loss: 0.560\n",
      "[9,   340] loss: 0.548\n",
      "[9,   360] loss: 0.541\n",
      "Epoch: 8, Val. loss: 0.54, Val. acc: 0.7\n",
      "[10,    20] loss: 0.533\n",
      "[10,    40] loss: 0.544\n",
      "[10,    60] loss: 0.540\n",
      "[10,    80] loss: 0.522\n",
      "[10,   100] loss: 0.540\n",
      "[10,   120] loss: 0.513\n",
      "[10,   140] loss: 0.538\n",
      "[10,   160] loss: 0.523\n",
      "[10,   180] loss: 0.518\n",
      "[10,   200] loss: 0.517\n",
      "[10,   220] loss: 0.529\n",
      "[10,   240] loss: 0.537\n",
      "[10,   260] loss: 0.549\n",
      "[10,   280] loss: 0.502\n",
      "[10,   300] loss: 0.535\n",
      "[10,   320] loss: 0.540\n",
      "[10,   340] loss: 0.522\n",
      "[10,   360] loss: 0.547\n",
      "Epoch: 9, Val. loss: 0.53, Val. acc: 0.71\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = GNN(D= 100, N = 70, F1 = 32 , F2 = 10, F = F1+F2).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay= 0.00001)\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, [XE, X, A,ESM1b, labels] in enumerate(train_loader):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        XE, X, A, ESM1b, labels = XE.to(device), X.to(device), A.to(device),ESM1b.to(device), labels.to(device)\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(XE, X, A, ESM1b)\n",
    "        loss = criterion(outputs, labels.view((batch_size,-1)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 20))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    #After each epoch, calculate the validation loss:\n",
    "    running_acc = 0.0\n",
    "    running_loss = 0.0\n",
    "    model.eval()\n",
    "    for i, [XE, X, A,ESM1b, labels] in enumerate(test_loader):\n",
    "        XE, X, A, ESM1b, labels = XE.to(device), X.to(device), A.to(device),ESM1b.to(device), labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(XE, X, A, ESM1b)\n",
    "        loss = criterion(outputs, labels.view((batch_size,-1)))\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        outputs2 = np.round(outputs.view(-1).cpu().detach().numpy()) \n",
    "        labels2 = labels.cpu().detach().numpy()\n",
    "        acc = np.mean([outputs2[i] == labels2[i] for i in range(len(labels))])\n",
    "        running_acc += acc\n",
    "\n",
    "    print(\"Epoch: %s, Val. loss: %s, Val. acc: %s\" % (epoch, np.round(running_loss/(i+1),2),\n",
    "                                                                  np.round(running_acc/(i+1), 2)))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),join(CURRENT_DIR, \"..\" ,\"data\", \"substrate_data\", \"GNN\", \"Pytorch_GNN_V2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 0.527\n",
      "[1,    40] loss: 0.524\n",
      "[1,    60] loss: 0.517\n",
      "[1,    80] loss: 0.511\n",
      "[1,   100] loss: 0.522\n",
      "[1,   120] loss: 0.519\n",
      "[1,   140] loss: 0.520\n",
      "[1,   160] loss: 0.507\n",
      "[1,   180] loss: 0.515\n",
      "[1,   200] loss: 0.526\n",
      "[1,   220] loss: 0.511\n",
      "[1,   240] loss: 0.527\n",
      "[1,   260] loss: 0.528\n",
      "[1,   280] loss: 0.522\n",
      "[1,   300] loss: 0.518\n",
      "[1,   320] loss: 0.524\n",
      "[1,   340] loss: 0.531\n",
      "[1,   360] loss: 0.545\n",
      "Epoch: 0, Val. loss: 0.54, Val. acc: 0.71\n",
      "[2,    20] loss: 0.536\n",
      "[2,    40] loss: 0.537\n",
      "[2,    60] loss: 0.502\n",
      "[2,    80] loss: 0.527\n",
      "[2,   100] loss: 0.544\n",
      "[2,   120] loss: 0.522\n",
      "[2,   140] loss: 0.519\n",
      "[2,   160] loss: 0.536\n",
      "[2,   180] loss: 0.522\n",
      "[2,   200] loss: 0.519\n",
      "[2,   220] loss: 0.519\n",
      "[2,   240] loss: 0.526\n",
      "[2,   260] loss: 0.500\n",
      "[2,   280] loss: 0.536\n",
      "[2,   300] loss: 0.507\n",
      "[2,   320] loss: 0.514\n",
      "[2,   340] loss: 0.527\n",
      "[2,   360] loss: 0.512\n",
      "Epoch: 1, Val. loss: 0.53, Val. acc: 0.72\n",
      "[3,    20] loss: 0.533\n",
      "[3,    40] loss: 0.522\n",
      "[3,    60] loss: 0.498\n",
      "[3,    80] loss: 0.493\n",
      "[3,   100] loss: 0.496\n",
      "[3,   120] loss: 0.518\n",
      "[3,   140] loss: 0.524\n",
      "[3,   160] loss: 0.508\n",
      "[3,   180] loss: 0.523\n",
      "[3,   200] loss: 0.510\n",
      "[3,   220] loss: 0.506\n",
      "[3,   240] loss: 0.509\n",
      "[3,   260] loss: 0.524\n",
      "[3,   280] loss: 0.519\n",
      "[3,   300] loss: 0.520\n",
      "[3,   320] loss: 0.516\n",
      "[3,   340] loss: 0.493\n",
      "[3,   360] loss: 0.531\n",
      "Epoch: 2, Val. loss: 0.52, Val. acc: 0.72\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, [XE, X, A,ESM1b, labels] in enumerate(train_loader):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        XE, X, A, ESM1b, labels = XE.to(device), X.to(device), A.to(device),ESM1b.to(device), labels.to(device)\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(XE, X, A, ESM1b)\n",
    "        loss = criterion(outputs, labels.view((batch_size,-1)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 20))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    #After each epoch, calculate the validation loss:\n",
    "    running_acc = 0.0\n",
    "    running_loss = 0.0\n",
    "    model.eval()\n",
    "    for i, [XE, X, A,ESM1b, labels] in enumerate(test_loader):\n",
    "        XE, X, A, ESM1b, labels = XE.to(device), X.to(device), A.to(device),ESM1b.to(device), labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(XE, X, A, ESM1b)\n",
    "        loss = criterion(outputs, labels.view((batch_size,-1)))\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        outputs2 = np.round(outputs.view(-1).cpu().detach().numpy()) \n",
    "        labels2 = labels.cpu().detach().numpy()\n",
    "        acc = np.mean([outputs2[i] == labels2[i] for i in range(len(labels))])\n",
    "        running_acc += acc\n",
    "\n",
    "    print(\"Epoch: %s, Val. loss: %s, Val. acc: %s\" % (epoch, np.round(running_loss/(i+1),2),\n",
    "                                                                  np.round(running_acc/(i+1), 2)))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNN(\n",
       "  (BN1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (BN2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (BN3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear1): Linear(in_features=100, out_features=32, bias=True)\n",
       "  (linear2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (drop_layer): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating GNN representations for traing and test set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNN(\n",
       "  (BN1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (BN2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (BN3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear1): Linear(in_features=100, out_features=32, bias=True)\n",
       "  (linear2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (drop_layer): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GNN(D= 50, N = 70, F1 = 32 , F2 = 10, F = F1+F2).to(device)\n",
    "model.load_state_dict(torch.load(join(CURRENT_DIR, \"..\" ,\"data\", \"substrate_data\", \"GNN\",\"Pytorch_GNN\")))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(join(CURRENT_DIR, \"..\" ,\"data\",\"splits\", \"df_train_with_ESM1b_ts.pkl\"))\n",
    "df_test = pd.read_pickle(join(CURRENT_DIR, \"..\" ,\"data\", \"splits\", \"df_test_with_ESM1b_ts.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a DataFrame with all metabolites in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule ID</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00001</td>\n",
       "      <td>P9WIQ3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00002</td>\n",
       "      <td>P9WIQ3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C00003</td>\n",
       "      <td>P9WIQ3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00004</td>\n",
       "      <td>P9WIQ3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C00005</td>\n",
       "      <td>P9WIQ3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>CHEBI_85986</td>\n",
       "      <td>P9WIQ3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>CHEBI_86339</td>\n",
       "      <td>P9WIQ3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>CHEBI_87136</td>\n",
       "      <td>P9WIQ3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>CHEBI_87305</td>\n",
       "      <td>P9WIQ3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>CHEBI_88052</td>\n",
       "      <td>P9WIQ3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1352 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      molecule ID     uid\n",
       "0          C00001  P9WIQ3\n",
       "1          C00002  P9WIQ3\n",
       "2          C00003  P9WIQ3\n",
       "3          C00004  P9WIQ3\n",
       "4          C00005  P9WIQ3\n",
       "...           ...     ...\n",
       "1347  CHEBI_85986  P9WIQ3\n",
       "1348  CHEBI_86339  P9WIQ3\n",
       "1349  CHEBI_87136  P9WIQ3\n",
       "1350  CHEBI_87305  P9WIQ3\n",
       "1351  CHEBI_88052  P9WIQ3\n",
       "\n",
       "[1352 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_mols = os.listdir(join(CURRENT_DIR, \"..\" ,\"data\", \"substrate_data\", \"GNN_input_matrices\"))\n",
    "valid_mols = [mol.split(\"_A.npy\")[0] for mol in valid_mols]\n",
    "valid_mols = [mol for mol in valid_mols if not \".\" in mol]\n",
    "df_mols = pd.DataFrame(data = {\"molecule ID\" : valid_mols})\n",
    "\n",
    "#To create the substrate rep, the UID does not matter. Therfore, setting it random:\n",
    "df_mols[\"uid\"] = \"P9WIQ3\"\n",
    "df_mols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_folder = join(CURRENT_DIR, \"..\" ,\"data\", \"substrate_data\", \"GNN_input_matrices\")\n",
    "\n",
    "def get_representation_input(cid_list):\n",
    "    XE = ();\n",
    "    X = ();\n",
    "    A = ();\n",
    "    UniRep = ();\n",
    "    extras = ();\n",
    "    # Generate data\n",
    "    for i in range(len(cid_list)):\n",
    "        cid  = cid_list[i]\n",
    "\n",
    "        X = X + (np.load(join(input_data_folder, cid + '_X.npy')), );\n",
    "        XE = XE + (np.load(join(input_data_folder, cid + '_XE.npy')), );\n",
    "        A = A + (np.load(join(input_data_folder, cid + '_A.npy')), );\n",
    "    return(XE, X, A)\n",
    "\n",
    "\n",
    "def get_substrate_representations(df):\n",
    "    df[\"substrate_rep\"] = \"\"\n",
    "    \n",
    "    i = 0\n",
    "    n = len(df)\n",
    "\n",
    "    while i*64 <= n:\n",
    "        cid_all = list(df[\"molecule ID\"])\n",
    "\n",
    "        if (i+1)*64  <= n:\n",
    "            XE, X, A= get_representation_input(cid_all[i*64:(i+1)*64])\n",
    "            \n",
    "            XE = torch.tensor(np.array(XE), dtype = torch.float32).to(device)\n",
    "            X = torch.tensor(np.array(X), dtype = torch.float32).to(device)\n",
    "            A = torch.tensor(np.array(A), dtype = torch.float32).to(device)\n",
    "            representations = model.get_GNN_rep(XE, X,A).cpu().detach().numpy()\n",
    "            df[\"substrate_rep\"][i*64:(i+1)*64] = list(representations[:, :D])\n",
    "        else:\n",
    "            print(i)\n",
    "            XE, X, A= get_representation_input(cid_all[i*64:(i+1)*64])\n",
    "            XE = torch.tensor(np.array(XE), dtype = torch.float32).to(device)\n",
    "            X = torch.tensor(np.array(X), dtype = torch.float32).to(device)\n",
    "            A = torch.tensor(np.array(A), dtype = torch.float32).to(device)\n",
    "            representations = model.get_GNN_rep(XE, X,A).cpu().detach().numpy()\n",
    "            df[\"substrate_rep\"][-len(representations):] = list(representations[:, :D])\n",
    "        i += 1\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule ID</th>\n",
       "      <th>uid</th>\n",
       "      <th>substrate_rep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00001</td>\n",
       "      <td>P9WIQ3</td>\n",
       "      <td>[0.029613253, 0.0, 0.26818, 0.0, 0.13194986, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00002</td>\n",
       "      <td>P9WIQ3</td>\n",
       "      <td>[1238.0188, 0.0, 0.0, 42.365837, 74.54658, 28....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C00003</td>\n",
       "      <td>P9WIQ3</td>\n",
       "      <td>[2111.6353, 0.0, 0.0, 139.63763, 183.79233, 33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00004</td>\n",
       "      <td>P9WIQ3</td>\n",
       "      <td>[1813.2203, 60.59075, 0.0, 224.62354, 335.9574...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C00005</td>\n",
       "      <td>P9WIQ3</td>\n",
       "      <td>[1859.3567, 120.4939, 0.0, 255.73376, 384.9909...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>CHEBI_85986</td>\n",
       "      <td>P9WIQ3</td>\n",
       "      <td>[673.1395, 31.783257, 0.0, 15.939333, 38.13377...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>CHEBI_86339</td>\n",
       "      <td>P9WIQ3</td>\n",
       "      <td>[6.3847322, 18.652742, 0.0, 76.061226, 61.6412...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>CHEBI_87136</td>\n",
       "      <td>P9WIQ3</td>\n",
       "      <td>[1872.2444, 96.23757, 0.0, 217.94662, 388.0197...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>CHEBI_87305</td>\n",
       "      <td>P9WIQ3</td>\n",
       "      <td>[1924.4116, 97.580894, 0.0, 217.94662, 384.225...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>CHEBI_88052</td>\n",
       "      <td>P9WIQ3</td>\n",
       "      <td>[82.04205, 2.6866338, 36.013027, 88.17541, 116...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1352 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      molecule ID     uid                                      substrate_rep\n",
       "0          C00001  P9WIQ3  [0.029613253, 0.0, 0.26818, 0.0, 0.13194986, 0...\n",
       "1          C00002  P9WIQ3  [1238.0188, 0.0, 0.0, 42.365837, 74.54658, 28....\n",
       "2          C00003  P9WIQ3  [2111.6353, 0.0, 0.0, 139.63763, 183.79233, 33...\n",
       "3          C00004  P9WIQ3  [1813.2203, 60.59075, 0.0, 224.62354, 335.9574...\n",
       "4          C00005  P9WIQ3  [1859.3567, 120.4939, 0.0, 255.73376, 384.9909...\n",
       "...           ...     ...                                                ...\n",
       "1347  CHEBI_85986  P9WIQ3  [673.1395, 31.783257, 0.0, 15.939333, 38.13377...\n",
       "1348  CHEBI_86339  P9WIQ3  [6.3847322, 18.652742, 0.0, 76.061226, 61.6412...\n",
       "1349  CHEBI_87136  P9WIQ3  [1872.2444, 96.23757, 0.0, 217.94662, 388.0197...\n",
       "1350  CHEBI_87305  P9WIQ3  [1924.4116, 97.580894, 0.0, 217.94662, 384.225...\n",
       "1351  CHEBI_88052  P9WIQ3  [82.04205, 2.6866338, 36.013027, 88.17541, 116...\n",
       "\n",
       "[1352 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mols = get_substrate_representations(df = df_mols)\n",
    "df_mols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexk\\AppData\\Local\\Temp/ipykernel_6620/2047707114.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"GNN rep\"][ind] = list(df_mols[\"substrate_rep\"].loc[df_mols[\"molecule ID\"] == df_train[\"molecule ID\"][ind].replace(\":\", \"_\")])[0]\n"
     ]
    }
   ],
   "source": [
    "df_train[\"GNN rep\"] = \"\"\n",
    "for ind in df_train.index:\n",
    "    try:\n",
    "        df_train[\"GNN rep\"][ind] = list(df_mols[\"substrate_rep\"].loc[df_mols[\"molecule ID\"] == df_train[\"molecule ID\"][ind].replace(\":\", \"_\")])[0]\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexk\\AppData\\Local\\Temp/ipykernel_6620/451796927.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"GNN rep\"][ind] = list(df_mols[\"substrate_rep\"].loc[df_mols[\"molecule ID\"] == df_test[\"molecule ID\"][ind].replace(\":\", \"_\")])[0]\n"
     ]
    }
   ],
   "source": [
    "df_test[\"GNN rep\"] = \"\"\n",
    "for ind in df_test.index:\n",
    "    try:\n",
    "        df_test[\"GNN rep\"][ind] = list(df_mols[\"substrate_rep\"].loc[df_mols[\"molecule ID\"] == df_test[\"molecule ID\"][ind].replace(\":\", \"_\")])[0]\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df_engqvist[\"molecule ID\"] = df_engqvist[\"substrate\"]\n",
    "\n",
    "df_engqvist[\"GNN rep\"] = \"\"\n",
    "for ind in df_engqvist.index:\n",
    "    try:\n",
    "        df_engqvist[\"GNN rep\"][ind] = list(df_mols[\"substrate_rep\"].loc[df_mols[\"molecule ID\"] == df_engqvist[\"molecule ID\"][ind].replace(\":\", \"_\")])[0]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "df_engqvist''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_pickle(join(CURRENT_DIR, \"..\" ,\"data\", \"splits\", \"df_train_with_ESM1b_ts_GNN.pkl\"), protocol = 4)\n",
    "df_test.to_pickle(join(CURRENT_DIR, \"..\" ,\"data\", \"splits\", \"df_test_with_ESM1b_ts_GNN.pkl\"), protocol = 4)\n",
    "\n",
    "#df_engqvist.to_pickle(join(CURRENT_DIR, \"alex_data\", \"new_test_data_Engqvist_group_with_GNN_train_test_split_with_GNN_similar.pkl\"), protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
