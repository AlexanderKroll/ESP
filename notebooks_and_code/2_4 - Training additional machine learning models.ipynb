{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training additional machine learning models for the task of enzyme-substrate pair prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and preprocessing data for model training and evaluation\n",
    "### 2. Training and validation machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexk\\anaconda3\\envs\\ESP\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexk\\projects\\ESP\\notebooks_and_code\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from os.path import join\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#from hyperopt import fmin, tpe, hp, Trials, rand\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "sys.path.append('.\\\\additional_code')\n",
    "#from data_preprocessing import *\n",
    "\n",
    "CURRENT_DIR = os.getcwd()\n",
    "print(CURRENT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and preprocessing data for model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_column_to_strings(df, column):\n",
    "    df[column] = [str(list(df[column][ind])) for ind in df.index]\n",
    "    return(df)\n",
    "\n",
    "def string_column_to_array(df, column):\n",
    "    df[column] = [np.array(eval(df[column][ind])) for ind in df.index]\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Loading data: \n",
    "Only keeping data points from the GO Annotation database with experimental evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexk\\anaconda3\\envs\\ESP\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:73: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n",
      "C:\\Users\\alexk\\anaconda3\\envs\\ESP\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:73: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_pickle(join(CURRENT_DIR, \"..\" ,\"data\",\"splits\", \"df_train_with_ESM1b_ts_GNN.pkl\"))\n",
    "df_train = df_train.loc[df_train[\"ESM1b\"] != \"\"]\n",
    "df_train = df_train.loc[df_train[\"type\"] != \"engqvist\"]\n",
    "df_train = df_train.loc[df_train[\"GNN rep\"] != \"\"]\n",
    "df_train.reset_index(inplace = True, drop = True)\n",
    "\n",
    "df_test  = pd.read_pickle(join(CURRENT_DIR, \"..\" ,\"data\",\"splits\", \"df_test_with_ESM1b_ts_GNN.pkl\"))\n",
    "df_test = df_test.loc[df_test[\"ESM1b\"] != \"\"]\n",
    "df_test = df_test.loc[df_test[\"type\"] != \"engqvist\"]\n",
    "df_test = df_test.loc[df_test[\"GNN rep\"] != \"\"]\n",
    "df_test.reset_index(inplace = True, drop = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Splitting training set into 5-folds for hyperparameter optimization:\n",
    "The 5 folds are created in such a way that the same enzyme does not occure in two different folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, frac):\n",
    "    df1 = pd.DataFrame(columns = list(df.columns))\n",
    "    df2 = pd.DataFrame(columns = list(df.columns))\n",
    "    try:\n",
    "        df.drop(columns = [\"level_0\"], inplace = True)\n",
    "    except: \n",
    "        pass\n",
    "    df.reset_index(inplace = True)\n",
    "    \n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    ind = 0\n",
    "    while len(train_indices) +len(test_indices) < len(df):\n",
    "        if ind not in train_indices and ind not in test_indices:\n",
    "            if ind % frac != 0:\n",
    "                n_old = len(train_indices)\n",
    "                train_indices.append(ind)\n",
    "                train_indices = list(set(train_indices))\n",
    "\n",
    "                while n_old != len(train_indices):\n",
    "                    n_old = len(train_indices)\n",
    "\n",
    "                    training_seqs= list(set(df[\"ESM1b\"].loc[train_indices]))\n",
    "\n",
    "                    train_indices = train_indices + (list(df.loc[df[\"ESM1b\"].isin(training_seqs)].index))\n",
    "                    train_indices = list(set(train_indices))\n",
    "                \n",
    "            else:\n",
    "                n_old = len(test_indices)\n",
    "                test_indices.append(ind)\n",
    "                test_indices = list(set(test_indices))\n",
    "\n",
    "                while n_old != len(test_indices):\n",
    "                    n_old = len(test_indices)\n",
    "\n",
    "                    testing_seqs= list(set(df[\"ESM1b\"].loc[test_indices]))\n",
    "\n",
    "                    test_indices = test_indices + (list(df.loc[df[\"ESM1b\"].isin(testing_seqs)].index))\n",
    "                    test_indices = list(set(test_indices))\n",
    "                \n",
    "        ind +=1\n",
    "    return(df.loc[train_indices], df.loc[test_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data_train2 = df_train.copy()\n",
    "data_train2 = array_column_to_strings(data_train2, column = \"ESM1b\")\n",
    "\n",
    "data_train2, df_fold = split_dataframe(df = data_train2, frac=5)\n",
    "indices_fold1 = list(df_fold[\"index\"])\n",
    "print(len(data_train2), len(indices_fold1))#\n",
    "\n",
    "data_train2, df_fold = split_dataframe(df = data_train2, frac=4)\n",
    "indices_fold2 = list(df_fold[\"index\"])\n",
    "print(len(data_train2), len(indices_fold2))\n",
    "\n",
    "data_train2, df_fold = split_dataframe(df = data_train2, frac=3)\n",
    "indices_fold3 = list(df_fold[\"index\"])\n",
    "print(len(data_train2), len(indices_fold3))\n",
    "\n",
    "data_train2, df_fold = split_dataframe(df = data_train2, frac=2)\n",
    "indices_fold4 = list(df_fold[\"index\"])\n",
    "indices_fold5 = list(data_train2[\"index\"])\n",
    "print(len(data_train2), len(indices_fold4))\n",
    "\n",
    "\n",
    "fold_indices = [indices_fold1, indices_fold2, indices_fold3, indices_fold4, indices_fold5]\n",
    "\n",
    "train_indices = [[], [], [], [], []]\n",
    "test_indices = [[], [], [], [], []]\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if i != j:\n",
    "            train_indices[i] = train_indices[i] + fold_indices[j]\n",
    "            \n",
    "    test_indices[i] = fold_indices[i]\n",
    "    \n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\",\"splits\", \"CV_train_indices.npy\"), train_indices)\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\",\"splits\", \"CV_test_indices.npy\"), test_indices)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = list(np.load(join(CURRENT_DIR, \"..\" ,\"data\",\"splits\", \"CV_train_indices.npy\"),  allow_pickle=True))\n",
    "test_indices = list(np.load(join(CURRENT_DIR, \"..\" ,\"data\",\"splits\", \"CV_test_indices.npy\"),  allow_pickle=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating numpy arrays with input vectors and output variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_and_output_data(df):\n",
    "    X = ();\n",
    "    y = ();\n",
    "    \n",
    "    for ind in df.index:\n",
    "        emb = df[\"ESM1b_ts\"][ind]\n",
    "        ecfp = np.array(list(df[\"ECFP\"][ind])).astype(int)\n",
    "                \n",
    "        X = X +(np.concatenate([ecfp, emb]), );\n",
    "        y = y + (df[\"Binding\"][ind], );\n",
    "\n",
    "    return(X,y)\n",
    "\n",
    "train_X, train_y =  create_input_and_output_data(df = df_train)\n",
    "test_X, test_y =  create_input_and_output_data(df = df_test)\n",
    "\n",
    "\n",
    "feature_names =  [\"ECFP_\" + str(i) for i in range(1024)]\n",
    "feature_names = feature_names + [\"ESM1b_ts_\" + str(i) for i in range(1280)]\n",
    "\n",
    "train_X = np.array(train_X)\n",
    "test_X  = np.array(test_X)\n",
    "\n",
    "train_y = np.array(train_y)\n",
    "test_y  = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(train_y)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_y)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "# normalize the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_X)\n",
    "train_X = scaler.transform(train_X)\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training and validation machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Performing hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_neg_acc_logistic_regression(param):\n",
    "    param['solver'] = 'liblinear'\n",
    "    param['class_weight'] = class_weights\n",
    "    \n",
    "    loss = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]        \n",
    "    \n",
    "        clf = LogisticRegression(max_iter=20, penalty = param[\"penalty\"],\n",
    "                                C = param[\"C\"], solver = param['solver'],\n",
    "                                class_weight= param[\"class_weight\"]\n",
    "                                ).fit(train_X[train_index], train_y[train_index])\n",
    "        y_valid_pred = np.round(clf.predict(train_X[test_index]))\n",
    "        validation_y = train_y[test_index]\n",
    "    \n",
    "        false_positive = 100*(1-np.mean(np.array(validation_y)[y_valid_pred == 1]))\n",
    "        false_negative = 100*(np.mean(np.array(validation_y)[y_valid_pred == 0]))\n",
    "        logging.info(\"False positive rate: \" + str(false_positive)+ \"; False negative rate: \" + str(false_negative))\n",
    "        loss.append(2*(false_negative**2) + false_positive**1.3)\n",
    "    return(np.mean(loss))\n",
    "\n",
    "#Defining search space for hyperparameter optimization\n",
    "space_logistic_regression = {'C': hp.choice('C', [1, 10, 100, 1000]),\n",
    "                            'penalty': hp.choice('penalty', ['l1', 'l2'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing a random grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''trials = Trials()\n",
    "\n",
    "for i in range(1,10):\n",
    "    best = fmin(fn = cross_validation_neg_acc_logistic_regression, space = space_logistic_regression,\n",
    "                algo = rand.suggest, max_evals = i, trials = trials)\n",
    "    print(i)\n",
    "    print(trials.best_trial[\"result\"][\"loss\"])\n",
    "    print(trials.argmin)''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best set of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\"C\" : 1,\n",
    "        \"penalty\" : \"l2\"}\n",
    "\n",
    "param['solver'] = 'liblinear'\n",
    "param['class_weight'] = class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Repeating 5-fold CV for best set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss values: [1481.1517290256324, 1527.9477878588439, 1610.891134117497, 1481.9223786505895, 1493.7427890128495]\n",
      "Accuracies: [0.555252882956506, 0.5611802674043338, 0.542320304883453, 0.5568644818423384, 0.5641326945674772]\n",
      "ROC-AUC scores: [0.5268393811621369, 0.5262103625271073, 0.5149750024593845, 0.5375482138817793, 0.5385738017600883]\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "accuracy = []\n",
    "ROC_AUC = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    clf = LogisticRegression(max_iter=250, penalty = param[\"penalty\"],\n",
    "                                C = param[\"C\"], solver = param['solver'],\n",
    "                                class_weight= param[\"class_weight\"]\n",
    "                                ).fit(train_X[train_index], train_y[train_index])\n",
    "    y_valid_pred = np.round(clf.predict(train_X[test_index]))\n",
    "    validation_y = train_y[test_index]\n",
    "\n",
    "    #calculate loss:\n",
    "    false_positive = 100*(1-np.mean(np.array(validation_y)[y_valid_pred == 1]))\n",
    "    false_negative = 100*(np.mean(np.array(validation_y)[y_valid_pred == 0]))\n",
    "    logging.info(\"False positive rate: \" + str(false_positive)+ \"; False negative rate: \" + str(false_negative))\n",
    "    loss.append(2*(false_negative**2) + false_positive**1.3)\n",
    "    #calculate accuracy:\n",
    "    .append(np.mean(y_valid_pred == np.array(validation_y)))\n",
    "    #calculate ROC-AUC score:\n",
    "    ROC_AUC.append(roc_auc_score(np.array(validation_y), clf.predict_proba(train_X[test_index])[:, 1]))\n",
    "    \n",
    "print(\"Loss values: %s\" %loss) \n",
    "print(\"Accuracies: %s\" %accuracy)\n",
    "print(\"ROC-AUC scores: %s\" %ROC_AUC)\n",
    "\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"acc_CV_LR.npy\"), np.array(accuracy))\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"loss_CV_LR.npy\"), np.array(loss))\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"ROC_AUC_CV_LR.npy\"), np.array(ROC_AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) 3. Training and validating the final model\n",
    "Training the model and validating it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.6304640718562874, ROC-AUC score for test set: 0.620806910231481, MCC: 0.13965916973823772\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=250, penalty = param[\"penalty\"],\n",
    "                                C = param[\"C\"], solver = param['solver'],\n",
    "                                class_weight= param[\"class_weight\"]\n",
    "                                ).fit(train_X, train_y)\n",
    "y_test_pred = np.round(clf.predict(test_X))\n",
    "acc_test = np.mean(y_test_pred == np.array(test_y))\n",
    "roc_auc = roc_auc_score(np.array(test_y), clf.predict_proba(test_X)[:, 1])\n",
    "mcc = matthews_corrcoef(np.array(test_y), y_test_pred)\n",
    "\n",
    "print(\"Accuracy on test set: %s, ROC-AUC score for test set: %s, MCC: %s\"  % (acc_test, roc_auc, mcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_neg_acc_random_forest(param):\n",
    "    param['solver'] = 'liblinear'\n",
    "    param['class_weight'] = class_weights\n",
    "    \n",
    "    loss = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]        \n",
    "    \n",
    "        clf = RandomForestClassifier(n_estimators = param[\"n_estimators\"],\n",
    "                                class_weight= param[\"class_weight\"]\n",
    "                                ).fit(train_X[train_index], train_y[train_index])\n",
    "        y_valid_pred = np.round(clf.predict(train_X[test_index]))\n",
    "        validation_y = train_y[test_index]\n",
    "    \n",
    "        false_positive = 100*(1-np.mean(np.array(validation_y)[y_valid_pred == 1]))\n",
    "        false_negative = 100*(np.mean(np.array(validation_y)[y_valid_pred == 0]))\n",
    "        logging.info(\"False positive rate: \" + str(false_positive)+ \"; False negative rate: \" + str(false_negative))\n",
    "        loss.append(2*(false_negative**2) + false_positive**1.3)\n",
    "    return(np.mean(loss))\n",
    "\n",
    "#Defining search space for hyperparameter optimization\n",
    "space_random_forest = {'n_estimators': hp.choice('n_estimators', [100, 200, 300])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing a random grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''trials = Trials()\n",
    "\n",
    "for i in range(1,10):\n",
    "    best = fmin(fn = cross_validation_neg_acc_random_forest, space = space_random_forest,\n",
    "                algo = rand.suggest, max_evals = i, trials = trials)\n",
    "    print(i)\n",
    "    print(trials.best_trial[\"result\"][\"loss\"])\n",
    "    print(trials.argmin)''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best set of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\"n_estimators\" : 100}\n",
    "param['class_weight'] = class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss values: [320.3317972277872, 297.5914956403973, 312.6594901020858, 278.87419374147123, 302.0886608020148]\n",
      "Accuracies: [0.8865885771361118, 0.8891655140617796, 0.8870867677036249, 0.8945084145261293, 0.8898128898128899]\n",
      "ROC-AUC scores: [0.9484840687096031, 0.9475283641094073, 0.9427751776258135, 0.9515115956023957, 0.9481888507479239]\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "accuracy = []\n",
    "ROC_AUC = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    clf = RandomForestClassifier(n_estimators = param[\"n_estimators\"],\n",
    "                                class_weight= param[\"class_weight\"]\n",
    "                                ).fit(train_X[train_index], train_y[train_index])\n",
    "    y_valid_pred = np.round(clf.predict(train_X[test_index]))\n",
    "    validation_y = train_y[test_index]\n",
    "\n",
    "    #calculate loss:\n",
    "    false_positive = 100*(1-np.mean(np.array(validation_y)[y_valid_pred == 1]))\n",
    "    false_negative = 100*(np.mean(np.array(validation_y)[y_valid_pred == 0]))\n",
    "    logging.info(\"False positive rate: \" + str(false_positive)+ \"; False negative rate: \" + str(false_negative))\n",
    "    loss.append(2*(false_negative**2) + false_positive**1.3)\n",
    "    #calculate accuracy:\n",
    "    accuracy.append(np.mean(y_valid_pred == np.array(validation_y)))\n",
    "    #calculate ROC-AUC score:\n",
    "    ROC_AUC.append(roc_auc_score(np.array(validation_y), clf.predict_proba(train_X[test_index])[:, 1]))\n",
    "    \n",
    "print(\"Loss values: %s\" %loss) \n",
    "print(\"Accuracies: %s\" %accuracy)\n",
    "print(\"ROC-AUC scores: %s\" %ROC_AUC)\n",
    "\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"acc_CV_random_forest.npy\"), np.array(accuracy))\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"loss_CV_random_forest.npy\"), np.array(loss))\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"ROC_AUC_CV_random_forest.npy\"), np.array(ROC_AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) 3. Training and validating the final model\n",
    "Training the model and validating it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.8771706586826348, ROC-AUC score for test set: 0.9445799929837202, MCC: 0.6703608017079266\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators = param[\"n_estimators\"],\n",
    "                                class_weight= param[\"class_weight\"]\n",
    "                                ).fit(train_X, train_y)\n",
    "y_test_pred = np.round(clf.predict(test_X))\n",
    "acc_test = np.mean(y_test_pred == np.array(test_y))\n",
    "roc_auc = roc_auc_score(np.array(test_y), clf.predict_proba(test_X)[:, 1])\n",
    "mcc = matthews_corrcoef(np.array(test_y), y_test_pred)\n",
    "\n",
    "print(\"Accuracy on test set: %s, ROC-AUC score for test set: %s, MCC: %s\"  % (acc_test, roc_auc, mcc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
