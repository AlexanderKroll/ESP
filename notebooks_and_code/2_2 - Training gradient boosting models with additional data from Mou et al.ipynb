{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training gradient boosting model for enzyme-substrate pair prediction with ESM-1b-vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and preprocessing data for model training and evaluation\n",
    "### 2. Hyperparameter optimization using a 5-fold cross-validation (CV)\n",
    "### 3. Training and validating the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexk\\Documents\\GitHub\\SubFinder\\notebooks_and_code\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from os.path import join\n",
    "from sklearn.model_selection import KFold\n",
    "#from hyperopt import fmin, tpe, hp, Trials, rand\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "sys.path.append('.\\\\additional_code')\n",
    "#from data_preprocessing import *\n",
    "\n",
    "CURRENT_DIR = os.getcwd()\n",
    "print(CURRENT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and preprocessing data for model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Loading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alexk\\anaconda3\\envs\\predicting_km\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:56: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n",
      "c:\\users\\alexk\\anaconda3\\envs\\predicting_km\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:56: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    }
   ],
   "source": [
    "df_test  = pd.read_pickle(join(CURRENT_DIR, \"..\" ,\"data\",\"splits\", \"df_test_with_ESM1b_ts.pkl\"))\n",
    "df_test = df_test.loc[df_test[\"ESM1b_ts\"] != \"\"]\n",
    "df_test.reset_index(inplace = True, drop = True)\n",
    "\n",
    "df_Mou  = pd.read_pickle(join(CURRENT_DIR, \"..\" ,\"data\", \"Mou_data\", \"Mou_df.pkl\"))\n",
    "df_Mou = df_Mou.loc[df_test[\"ESM1b_ts\"] != \"\"]\n",
    "df_Mou.reset_index(inplace = True, drop = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading new dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_and_output_data(df):\n",
    "    X = ();\n",
    "    y = ();\n",
    "    \n",
    "    for ind in df.index:\n",
    "        emb = df[\"ESM1b_ts\"][ind]\n",
    "        ecfp = np.array(list(df[\"ECFP\"][ind])).astype(int)\n",
    "                \n",
    "        X = X +(np.concatenate([ecfp, emb]), );\n",
    "        y = y + (df[\"Binding\"][ind], );\n",
    "\n",
    "    return(np.array(X),np.array(y))\n",
    "\n",
    "feature_names =  [\"ECFP_\" + str(i) for i in range(1024)]\n",
    "feature_names = feature_names + [\"ESM1b_ts_\" + str(i) for i in range(1280)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Mou et al. data to the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alexk\\anaconda3\\envs\\predicting_km\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:56: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train = pd.read_pickle(join(CURRENT_DIR, \"..\" ,\"data\",\n",
    "                               \"splits\", \"df_train_with_ESM1b_ts.pkl\"))\n",
    "df_train = df_train.loc[df_train[\"ESM1b_ts\"] != \"\"]\n",
    "df_train.reset_index(inplace = True, drop = True)\n",
    "\n",
    "\n",
    "train_X, train_y =  create_input_and_output_data(df = df_train)\n",
    "test_X, test_y =  create_input_and_output_data(df = df_test)\n",
    "\n",
    "df_test_new = df_Mou.copy()\n",
    "df_test_new[\"Binding\"] = [y > 2 for y in df_test_new[\"activity\"]]\n",
    "test_new_X, test_new_y =  create_input_and_output_data(df = df_test_new)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#same split as in Mou et al paper:\n",
    "X_train_Mou, X_test_Mou, y_train_Mou, y_test_Mou = train_test_split(test_new_X, test_new_y,\n",
    "                                                                    test_size = 0.20, random_state = 888)\n",
    "\n",
    "train_X = np.concatenate([train_X, X_train_Mou])\n",
    "train_y = np.concatenate([train_y, y_train_Mou])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:24:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\teval-logloss:0.69533\ttrain-logloss:0.60050\n",
      "[1]\teval-logloss:0.68843\ttrain-logloss:0.53780\n",
      "[2]\teval-logloss:0.69631\ttrain-logloss:0.50153\n",
      "[3]\teval-logloss:0.69656\ttrain-logloss:0.47159\n",
      "[4]\teval-logloss:0.66151\ttrain-logloss:0.44987\n",
      "[5]\teval-logloss:0.66181\ttrain-logloss:0.43055\n",
      "[6]\teval-logloss:0.64271\ttrain-logloss:0.41778\n",
      "[7]\teval-logloss:0.65235\ttrain-logloss:0.40439\n",
      "[8]\teval-logloss:0.61257\ttrain-logloss:0.38855\n",
      "[9]\teval-logloss:0.59625\ttrain-logloss:0.37997\n",
      "[10]\teval-logloss:0.55848\ttrain-logloss:0.36813\n",
      "[11]\teval-logloss:0.54859\ttrain-logloss:0.36277\n",
      "[12]\teval-logloss:0.54421\ttrain-logloss:0.35815\n",
      "[13]\teval-logloss:0.53491\ttrain-logloss:0.35172\n",
      "[14]\teval-logloss:0.53478\ttrain-logloss:0.34750\n",
      "[15]\teval-logloss:0.52499\ttrain-logloss:0.34362\n",
      "[16]\teval-logloss:0.51831\ttrain-logloss:0.33721\n",
      "[17]\teval-logloss:0.51682\ttrain-logloss:0.33202\n",
      "[18]\teval-logloss:0.51769\ttrain-logloss:0.32736\n",
      "[19]\teval-logloss:0.51469\ttrain-logloss:0.31924\n",
      "[20]\teval-logloss:0.51767\ttrain-logloss:0.31460\n",
      "[21]\teval-logloss:0.51872\ttrain-logloss:0.30876\n",
      "[22]\teval-logloss:0.52074\ttrain-logloss:0.30366\n",
      "[23]\teval-logloss:0.52119\ttrain-logloss:0.29758\n",
      "[24]\teval-logloss:0.52041\ttrain-logloss:0.29382\n",
      "[25]\teval-logloss:0.50735\ttrain-logloss:0.28973\n",
      "[26]\teval-logloss:0.49891\ttrain-logloss:0.28713\n",
      "[27]\teval-logloss:0.49357\ttrain-logloss:0.28306\n",
      "[28]\teval-logloss:0.49369\ttrain-logloss:0.27855\n",
      "[29]\teval-logloss:0.50584\ttrain-logloss:0.27621\n",
      "[30]\teval-logloss:0.50320\ttrain-logloss:0.27367\n",
      "[31]\teval-logloss:0.50005\ttrain-logloss:0.27141\n",
      "[32]\teval-logloss:0.50052\ttrain-logloss:0.26906\n",
      "[33]\teval-logloss:0.50084\ttrain-logloss:0.26648\n",
      "[34]\teval-logloss:0.50184\ttrain-logloss:0.26322\n",
      "[35]\teval-logloss:0.50183\ttrain-logloss:0.26132\n",
      "[36]\teval-logloss:0.50148\ttrain-logloss:0.25754\n",
      "[37]\teval-logloss:0.50159\ttrain-logloss:0.25475\n",
      "[38]\teval-logloss:0.50252\ttrain-logloss:0.25240\n",
      "[39]\teval-logloss:0.50267\ttrain-logloss:0.24904\n",
      "[40]\teval-logloss:0.50260\ttrain-logloss:0.24662\n",
      "[41]\teval-logloss:0.50479\ttrain-logloss:0.24393\n",
      "[42]\teval-logloss:0.50935\ttrain-logloss:0.24113\n",
      "[43]\teval-logloss:0.50910\ttrain-logloss:0.23843\n",
      "[44]\teval-logloss:0.50377\ttrain-logloss:0.23707\n",
      "[45]\teval-logloss:0.50907\ttrain-logloss:0.23390\n",
      "[46]\teval-logloss:0.50944\ttrain-logloss:0.23121\n",
      "[47]\teval-logloss:0.50866\ttrain-logloss:0.22961\n",
      "[48]\teval-logloss:0.50916\ttrain-logloss:0.22689\n",
      "[49]\teval-logloss:0.50451\ttrain-logloss:0.22437\n",
      "[50]\teval-logloss:0.50063\ttrain-logloss:0.22176\n",
      "[51]\teval-logloss:0.49812\ttrain-logloss:0.21965\n",
      "[52]\teval-logloss:0.49565\ttrain-logloss:0.21726\n",
      "[53]\teval-logloss:0.49459\ttrain-logloss:0.21397\n",
      "[54]\teval-logloss:0.49006\ttrain-logloss:0.21066\n",
      "[55]\teval-logloss:0.49049\ttrain-logloss:0.20789\n",
      "[56]\teval-logloss:0.48706\ttrain-logloss:0.20572\n",
      "[57]\teval-logloss:0.48675\ttrain-logloss:0.20454\n",
      "[58]\teval-logloss:0.48627\ttrain-logloss:0.20230\n",
      "[59]\teval-logloss:0.48574\ttrain-logloss:0.19835\n",
      "[60]\teval-logloss:0.48156\ttrain-logloss:0.19633\n",
      "[61]\teval-logloss:0.48144\ttrain-logloss:0.19520\n",
      "[62]\teval-logloss:0.46054\ttrain-logloss:0.19401\n",
      "[63]\teval-logloss:0.46016\ttrain-logloss:0.19110\n",
      "[64]\teval-logloss:0.46017\ttrain-logloss:0.18903\n",
      "[65]\teval-logloss:0.46007\ttrain-logloss:0.18695\n",
      "[66]\teval-logloss:0.45984\ttrain-logloss:0.18511\n",
      "[67]\teval-logloss:0.45696\ttrain-logloss:0.18345\n",
      "[68]\teval-logloss:0.45639\ttrain-logloss:0.18221\n",
      "[69]\teval-logloss:0.44187\ttrain-logloss:0.18083\n",
      "[70]\teval-logloss:0.44246\ttrain-logloss:0.17926\n",
      "[71]\teval-logloss:0.44301\ttrain-logloss:0.17749\n",
      "[72]\teval-logloss:0.44334\ttrain-logloss:0.17606\n",
      "[73]\teval-logloss:0.42202\ttrain-logloss:0.17448\n",
      "[74]\teval-logloss:0.42233\ttrain-logloss:0.17247\n",
      "[75]\teval-logloss:0.42178\ttrain-logloss:0.17092\n",
      "[76]\teval-logloss:0.41325\ttrain-logloss:0.16945\n",
      "[77]\teval-logloss:0.41308\ttrain-logloss:0.16749\n",
      "[78]\teval-logloss:0.41085\ttrain-logloss:0.16565\n",
      "[79]\teval-logloss:0.40945\ttrain-logloss:0.16372\n",
      "[80]\teval-logloss:0.40062\ttrain-logloss:0.16250\n",
      "[81]\teval-logloss:0.40123\ttrain-logloss:0.16094\n",
      "[82]\teval-logloss:0.40141\ttrain-logloss:0.15970\n",
      "[83]\teval-logloss:0.39563\ttrain-logloss:0.15860\n",
      "[84]\teval-logloss:0.39581\ttrain-logloss:0.15751\n",
      "[85]\teval-logloss:0.39587\ttrain-logloss:0.15653\n",
      "[86]\teval-logloss:0.39458\ttrain-logloss:0.15524\n",
      "[87]\teval-logloss:0.39417\ttrain-logloss:0.15415\n",
      "[88]\teval-logloss:0.39050\ttrain-logloss:0.15323\n",
      "[89]\teval-logloss:0.38203\ttrain-logloss:0.15214\n",
      "[90]\teval-logloss:0.37934\ttrain-logloss:0.15131\n",
      "[91]\teval-logloss:0.36989\ttrain-logloss:0.15013\n",
      "[92]\teval-logloss:0.36786\ttrain-logloss:0.14945\n",
      "[93]\teval-logloss:0.37115\ttrain-logloss:0.14831\n",
      "[94]\teval-logloss:0.37122\ttrain-logloss:0.14738\n",
      "[95]\teval-logloss:0.37094\ttrain-logloss:0.14646\n",
      "[96]\teval-logloss:0.37196\ttrain-logloss:0.14590\n",
      "[97]\teval-logloss:0.37090\ttrain-logloss:0.14481\n",
      "[98]\teval-logloss:0.37786\ttrain-logloss:0.14337\n",
      "[99]\teval-logloss:0.36518\ttrain-logloss:0.14266\n",
      "[100]\teval-logloss:0.36517\ttrain-logloss:0.14177\n",
      "[101]\teval-logloss:0.36500\ttrain-logloss:0.14110\n",
      "[102]\teval-logloss:0.36501\ttrain-logloss:0.13967\n",
      "[103]\teval-logloss:0.36423\ttrain-logloss:0.13875\n",
      "[104]\teval-logloss:0.36478\ttrain-logloss:0.13817\n",
      "[105]\teval-logloss:0.36182\ttrain-logloss:0.13730\n",
      "[106]\teval-logloss:0.36107\ttrain-logloss:0.13626\n",
      "[107]\teval-logloss:0.36000\ttrain-logloss:0.13560\n",
      "[108]\teval-logloss:0.36246\ttrain-logloss:0.13413\n",
      "[109]\teval-logloss:0.36178\ttrain-logloss:0.13332\n",
      "[110]\teval-logloss:0.36241\ttrain-logloss:0.13239\n",
      "[111]\teval-logloss:0.35915\ttrain-logloss:0.13129\n",
      "[112]\teval-logloss:0.35914\ttrain-logloss:0.13020\n",
      "[113]\teval-logloss:0.35537\ttrain-logloss:0.12850\n",
      "[114]\teval-logloss:0.35619\ttrain-logloss:0.12749\n",
      "[115]\teval-logloss:0.35435\ttrain-logloss:0.12654\n",
      "[116]\teval-logloss:0.35391\ttrain-logloss:0.12579\n",
      "[117]\teval-logloss:0.35310\ttrain-logloss:0.12540\n",
      "[118]\teval-logloss:0.36009\ttrain-logloss:0.12426\n",
      "[119]\teval-logloss:0.35980\ttrain-logloss:0.12311\n",
      "[120]\teval-logloss:0.36109\ttrain-logloss:0.12217\n",
      "[121]\teval-logloss:0.36023\ttrain-logloss:0.12138\n",
      "[122]\teval-logloss:0.36025\ttrain-logloss:0.12051\n",
      "[123]\teval-logloss:0.36026\ttrain-logloss:0.11972\n",
      "[124]\teval-logloss:0.35978\ttrain-logloss:0.11887\n",
      "[125]\teval-logloss:0.36119\ttrain-logloss:0.11842\n",
      "[126]\teval-logloss:0.35896\ttrain-logloss:0.11757\n",
      "[127]\teval-logloss:0.35842\ttrain-logloss:0.11684\n",
      "[128]\teval-logloss:0.35902\ttrain-logloss:0.11617\n",
      "[129]\teval-logloss:0.35895\ttrain-logloss:0.11534\n",
      "[130]\teval-logloss:0.35901\ttrain-logloss:0.11462\n",
      "[131]\teval-logloss:0.35803\ttrain-logloss:0.11391\n",
      "[132]\teval-logloss:0.35788\ttrain-logloss:0.11348\n",
      "[133]\teval-logloss:0.36099\ttrain-logloss:0.11285\n",
      "[134]\teval-logloss:0.36105\ttrain-logloss:0.11208\n",
      "[135]\teval-logloss:0.36419\ttrain-logloss:0.11148\n",
      "[136]\teval-logloss:0.36394\ttrain-logloss:0.11088\n",
      "[137]\teval-logloss:0.36346\ttrain-logloss:0.11056\n",
      "[138]\teval-logloss:0.37130\ttrain-logloss:0.10952\n",
      "[139]\teval-logloss:0.37144\ttrain-logloss:0.10868\n",
      "[140]\teval-logloss:0.37144\ttrain-logloss:0.10749\n",
      "[141]\teval-logloss:0.37073\ttrain-logloss:0.10685\n",
      "[142]\teval-logloss:0.37065\ttrain-logloss:0.10617\n",
      "[143]\teval-logloss:0.37184\ttrain-logloss:0.10521\n",
      "[144]\teval-logloss:0.37171\ttrain-logloss:0.10465\n",
      "[145]\teval-logloss:0.36654\ttrain-logloss:0.10392\n",
      "[146]\teval-logloss:0.36472\ttrain-logloss:0.10339\n",
      "[147]\teval-logloss:0.36402\ttrain-logloss:0.10289\n",
      "[148]\teval-logloss:0.35835\ttrain-logloss:0.10245\n",
      "[149]\teval-logloss:0.35718\ttrain-logloss:0.10131\n",
      "[150]\teval-logloss:0.35581\ttrain-logloss:0.10093\n",
      "[151]\teval-logloss:0.35456\ttrain-logloss:0.10039\n",
      "[152]\teval-logloss:0.35249\ttrain-logloss:0.09985\n",
      "[153]\teval-logloss:0.35435\ttrain-logloss:0.09937\n",
      "[154]\teval-logloss:0.35420\ttrain-logloss:0.09872\n",
      "[155]\teval-logloss:0.35424\ttrain-logloss:0.09816\n",
      "[156]\teval-logloss:0.35475\ttrain-logloss:0.09764\n",
      "[157]\teval-logloss:0.35445\ttrain-logloss:0.09710\n",
      "[158]\teval-logloss:0.35201\ttrain-logloss:0.09650\n",
      "[159]\teval-logloss:0.34953\ttrain-logloss:0.09607\n",
      "[160]\teval-logloss:0.35510\ttrain-logloss:0.09551\n",
      "[161]\teval-logloss:0.35483\ttrain-logloss:0.09503\n",
      "[162]\teval-logloss:0.35464\ttrain-logloss:0.09453\n",
      "[163]\teval-logloss:0.35678\ttrain-logloss:0.09417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[164]\teval-logloss:0.34977\ttrain-logloss:0.09392\n",
      "[165]\teval-logloss:0.34954\ttrain-logloss:0.09295\n",
      "[166]\teval-logloss:0.34388\ttrain-logloss:0.09255\n",
      "[167]\teval-logloss:0.34417\ttrain-logloss:0.09213\n",
      "[168]\teval-logloss:0.34417\ttrain-logloss:0.09174\n",
      "[169]\teval-logloss:0.34357\ttrain-logloss:0.09148\n",
      "[170]\teval-logloss:0.34277\ttrain-logloss:0.09103\n",
      "[171]\teval-logloss:0.34068\ttrain-logloss:0.09071\n",
      "[172]\teval-logloss:0.33902\ttrain-logloss:0.09032\n",
      "[173]\teval-logloss:0.33729\ttrain-logloss:0.08982\n",
      "[174]\teval-logloss:0.33502\ttrain-logloss:0.08938\n",
      "[175]\teval-logloss:0.33413\ttrain-logloss:0.08892\n",
      "[176]\teval-logloss:0.33417\ttrain-logloss:0.08755\n",
      "[177]\teval-logloss:0.33417\ttrain-logloss:0.08705\n",
      "[178]\teval-logloss:0.33422\ttrain-logloss:0.08654\n",
      "[179]\teval-logloss:0.33422\ttrain-logloss:0.08618\n",
      "[180]\teval-logloss:0.33254\ttrain-logloss:0.08574\n",
      "[181]\teval-logloss:0.33254\ttrain-logloss:0.08534\n",
      "[182]\teval-logloss:0.33486\ttrain-logloss:0.08503\n",
      "[183]\teval-logloss:0.34113\ttrain-logloss:0.08442\n",
      "[184]\teval-logloss:0.34081\ttrain-logloss:0.08405\n",
      "[185]\teval-logloss:0.34090\ttrain-logloss:0.08311\n",
      "[186]\teval-logloss:0.34131\ttrain-logloss:0.08264\n",
      "[187]\teval-logloss:0.34127\ttrain-logloss:0.08214\n",
      "[188]\teval-logloss:0.34075\ttrain-logloss:0.08157\n",
      "[189]\teval-logloss:0.33921\ttrain-logloss:0.08118\n",
      "[190]\teval-logloss:0.34136\ttrain-logloss:0.08045\n",
      "[191]\teval-logloss:0.34167\ttrain-logloss:0.07985\n",
      "[192]\teval-logloss:0.34177\ttrain-logloss:0.07952\n",
      "[193]\teval-logloss:0.34037\ttrain-logloss:0.07917\n",
      "[194]\teval-logloss:0.34036\ttrain-logloss:0.07885\n",
      "[195]\teval-logloss:0.33973\ttrain-logloss:0.07837\n",
      "[196]\teval-logloss:0.33970\ttrain-logloss:0.07804\n",
      "[197]\teval-logloss:0.34016\ttrain-logloss:0.07784\n",
      "[198]\teval-logloss:0.34007\ttrain-logloss:0.07745\n",
      "[199]\teval-logloss:0.34071\ttrain-logloss:0.07715\n",
      "[200]\teval-logloss:0.34071\ttrain-logloss:0.07679\n",
      "[201]\teval-logloss:0.34081\ttrain-logloss:0.07646\n",
      "[202]\teval-logloss:0.34070\ttrain-logloss:0.07614\n",
      "[203]\teval-logloss:0.34071\ttrain-logloss:0.07568\n",
      "[204]\teval-logloss:0.33680\ttrain-logloss:0.07554\n",
      "[205]\teval-logloss:0.33686\ttrain-logloss:0.07503\n",
      "[206]\teval-logloss:0.33500\ttrain-logloss:0.07449\n",
      "[207]\teval-logloss:0.33649\ttrain-logloss:0.07412\n",
      "[208]\teval-logloss:0.33190\ttrain-logloss:0.07361\n",
      "[209]\teval-logloss:0.33170\ttrain-logloss:0.07342\n",
      "[210]\teval-logloss:0.33154\ttrain-logloss:0.07304\n",
      "[211]\teval-logloss:0.33144\ttrain-logloss:0.07261\n",
      "[212]\teval-logloss:0.33155\ttrain-logloss:0.07230\n",
      "[213]\teval-logloss:0.33433\ttrain-logloss:0.07207\n",
      "[214]\teval-logloss:0.33464\ttrain-logloss:0.07162\n",
      "[215]\teval-logloss:0.33289\ttrain-logloss:0.07135\n",
      "[216]\teval-logloss:0.33276\ttrain-logloss:0.07119\n",
      "[217]\teval-logloss:0.33257\ttrain-logloss:0.07087\n",
      "[218]\teval-logloss:0.33225\ttrain-logloss:0.07044\n",
      "[219]\teval-logloss:0.32867\ttrain-logloss:0.07006\n",
      "[220]\teval-logloss:0.32646\ttrain-logloss:0.06971\n",
      "[221]\teval-logloss:0.33147\ttrain-logloss:0.06948\n",
      "[222]\teval-logloss:0.33147\ttrain-logloss:0.06929\n",
      "[223]\teval-logloss:0.33001\ttrain-logloss:0.06887\n",
      "[224]\teval-logloss:0.33028\ttrain-logloss:0.06858\n",
      "[225]\teval-logloss:0.32985\ttrain-logloss:0.06806\n",
      "[226]\teval-logloss:0.32985\ttrain-logloss:0.06763\n",
      "[227]\teval-logloss:0.32699\ttrain-logloss:0.06713\n",
      "[228]\teval-logloss:0.32518\ttrain-logloss:0.06690\n",
      "[229]\teval-logloss:0.32380\ttrain-logloss:0.06650\n",
      "[230]\teval-logloss:0.32244\ttrain-logloss:0.06607\n",
      "[231]\teval-logloss:0.32172\ttrain-logloss:0.06570\n",
      "[232]\teval-logloss:0.32042\ttrain-logloss:0.06532\n",
      "[233]\teval-logloss:0.32002\ttrain-logloss:0.06517\n",
      "[234]\teval-logloss:0.32002\ttrain-logloss:0.06487\n",
      "[235]\teval-logloss:0.32002\ttrain-logloss:0.06458\n",
      "[236]\teval-logloss:0.31894\ttrain-logloss:0.06441\n",
      "[237]\teval-logloss:0.31844\ttrain-logloss:0.06428\n",
      "[238]\teval-logloss:0.31844\ttrain-logloss:0.06395\n",
      "[239]\teval-logloss:0.31844\ttrain-logloss:0.06369\n",
      "[240]\teval-logloss:0.31790\ttrain-logloss:0.06336\n",
      "[241]\teval-logloss:0.31790\ttrain-logloss:0.06310\n",
      "[242]\teval-logloss:0.31790\ttrain-logloss:0.06283\n",
      "[243]\teval-logloss:0.31384\ttrain-logloss:0.06249\n",
      "[244]\teval-logloss:0.31368\ttrain-logloss:0.06230\n",
      "[245]\teval-logloss:0.31376\ttrain-logloss:0.06205\n",
      "[246]\teval-logloss:0.31209\ttrain-logloss:0.06161\n",
      "[247]\teval-logloss:0.31422\ttrain-logloss:0.06137\n",
      "[248]\teval-logloss:0.31422\ttrain-logloss:0.06114\n",
      "[249]\teval-logloss:0.31279\ttrain-logloss:0.06091\n",
      "[250]\teval-logloss:0.31056\ttrain-logloss:0.06063\n",
      "[251]\teval-logloss:0.31056\ttrain-logloss:0.06043\n",
      "[252]\teval-logloss:0.31074\ttrain-logloss:0.06028\n",
      "[253]\teval-logloss:0.31115\ttrain-logloss:0.06010\n",
      "[254]\teval-logloss:0.31301\ttrain-logloss:0.05985\n",
      "[255]\teval-logloss:0.31326\ttrain-logloss:0.05954\n",
      "[256]\teval-logloss:0.31268\ttrain-logloss:0.05922\n",
      "[257]\teval-logloss:0.31012\ttrain-logloss:0.05881\n",
      "[258]\teval-logloss:0.30956\ttrain-logloss:0.05860\n",
      "[259]\teval-logloss:0.30975\ttrain-logloss:0.05837\n",
      "[260]\teval-logloss:0.30841\ttrain-logloss:0.05815\n",
      "[261]\teval-logloss:0.30820\ttrain-logloss:0.05800\n",
      "[262]\teval-logloss:0.30799\ttrain-logloss:0.05768\n",
      "[263]\teval-logloss:0.30542\ttrain-logloss:0.05734\n",
      "[264]\teval-logloss:0.30424\ttrain-logloss:0.05712\n",
      "[265]\teval-logloss:0.30580\ttrain-logloss:0.05686\n",
      "[266]\teval-logloss:0.30585\ttrain-logloss:0.05669\n",
      "[267]\teval-logloss:0.30726\ttrain-logloss:0.05650\n",
      "[268]\teval-logloss:0.30719\ttrain-logloss:0.05627\n",
      "[269]\teval-logloss:0.30567\ttrain-logloss:0.05611\n",
      "[270]\teval-logloss:0.30524\ttrain-logloss:0.05588\n",
      "[271]\teval-logloss:0.30616\ttrain-logloss:0.05562\n",
      "[272]\teval-logloss:0.30581\ttrain-logloss:0.05544\n",
      "[273]\teval-logloss:0.30580\ttrain-logloss:0.05523\n",
      "[274]\teval-logloss:0.30693\ttrain-logloss:0.05504\n",
      "[275]\teval-logloss:0.30706\ttrain-logloss:0.05486\n",
      "[276]\teval-logloss:0.30704\ttrain-logloss:0.05459\n",
      "[277]\teval-logloss:0.30391\ttrain-logloss:0.05440\n",
      "[278]\teval-logloss:0.30421\ttrain-logloss:0.05411\n",
      "[279]\teval-logloss:0.30601\ttrain-logloss:0.05385\n",
      "[280]\teval-logloss:0.30516\ttrain-logloss:0.05367\n",
      "[281]\teval-logloss:0.30673\ttrain-logloss:0.05348\n",
      "[282]\teval-logloss:0.30672\ttrain-logloss:0.05332\n",
      "[283]\teval-logloss:0.31039\ttrain-logloss:0.05315\n",
      "[284]\teval-logloss:0.31093\ttrain-logloss:0.05300\n",
      "[285]\teval-logloss:0.31078\ttrain-logloss:0.05272\n",
      "[286]\teval-logloss:0.30913\ttrain-logloss:0.05245\n",
      "[287]\teval-logloss:0.30876\ttrain-logloss:0.05226\n",
      "[288]\teval-logloss:0.31186\ttrain-logloss:0.05209\n",
      "[289]\teval-logloss:0.31187\ttrain-logloss:0.05190\n",
      "[290]\teval-logloss:0.31216\ttrain-logloss:0.05176\n",
      "[291]\teval-logloss:0.31126\ttrain-logloss:0.05147\n",
      "[292]\teval-logloss:0.31126\ttrain-logloss:0.05136\n",
      "[293]\teval-logloss:0.31022\ttrain-logloss:0.05120\n",
      "[294]\teval-logloss:0.30807\ttrain-logloss:0.05089\n",
      "[295]\teval-logloss:0.31005\ttrain-logloss:0.05068\n",
      "[296]\teval-logloss:0.31015\ttrain-logloss:0.05047\n",
      "[297]\teval-logloss:0.31053\ttrain-logloss:0.05033\n",
      "[298]\teval-logloss:0.31314\ttrain-logloss:0.05005\n",
      "[299]\teval-logloss:0.31437\ttrain-logloss:0.04981\n",
      "[300]\teval-logloss:0.31370\ttrain-logloss:0.04959\n",
      "[301]\teval-logloss:0.31396\ttrain-logloss:0.04945\n",
      "[302]\teval-logloss:0.31344\ttrain-logloss:0.04930\n",
      "[303]\teval-logloss:0.31450\ttrain-logloss:0.04897\n",
      "[304]\teval-logloss:0.31617\ttrain-logloss:0.04871\n",
      "[305]\teval-logloss:0.31629\ttrain-logloss:0.04857\n",
      "[306]\teval-logloss:0.31518\ttrain-logloss:0.04836\n",
      "[307]\teval-logloss:0.31512\ttrain-logloss:0.04822\n",
      "[308]\teval-logloss:0.31287\ttrain-logloss:0.04808\n",
      "[309]\teval-logloss:0.31223\ttrain-logloss:0.04789\n",
      "[310]\teval-logloss:0.30960\ttrain-logloss:0.04772\n",
      "[311]\teval-logloss:0.30950\ttrain-logloss:0.04757\n",
      "[312]\teval-logloss:0.30874\ttrain-logloss:0.04734\n",
      "[313]\teval-logloss:0.30802\ttrain-logloss:0.04718\n",
      "[314]\teval-logloss:0.30783\ttrain-logloss:0.04707\n",
      "[315]\teval-logloss:0.30744\ttrain-logloss:0.04688\n",
      "[316]\teval-logloss:0.30740\ttrain-logloss:0.04669\n",
      "[317]\teval-logloss:0.30389\ttrain-logloss:0.04650\n",
      "[318]\teval-logloss:0.30452\ttrain-logloss:0.04635\n",
      "[319]\teval-logloss:0.30464\ttrain-logloss:0.04621\n",
      "[320]\teval-logloss:0.30474\ttrain-logloss:0.04600\n",
      "[321]\teval-logloss:0.30546\ttrain-logloss:0.04589\n",
      "[322]\teval-logloss:0.30416\ttrain-logloss:0.04565\n",
      "[323]\teval-logloss:0.30417\ttrain-logloss:0.04555\n",
      "[324]\teval-logloss:0.30397\ttrain-logloss:0.04539\n",
      "[325]\teval-logloss:0.30402\ttrain-logloss:0.04523\n",
      "[326]\teval-logloss:0.30408\ttrain-logloss:0.04512\n",
      "[327]\teval-logloss:0.30429\ttrain-logloss:0.04493\n",
      "[328]\teval-logloss:0.30488\ttrain-logloss:0.04467\n",
      "[329]\teval-logloss:0.30490\ttrain-logloss:0.04451\n",
      "[330]\teval-logloss:0.30492\ttrain-logloss:0.04438\n",
      "[331]\teval-logloss:0.30961\ttrain-logloss:0.04423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[332]\teval-logloss:0.30960\ttrain-logloss:0.04404\n",
      "[333]\teval-logloss:0.30854\ttrain-logloss:0.04392\n",
      "[334]\teval-logloss:0.30917\ttrain-logloss:0.04368\n",
      "[335]\teval-logloss:0.30907\ttrain-logloss:0.04360\n",
      "[336]\teval-logloss:0.31012\ttrain-logloss:0.04344\n",
      "[337]\teval-logloss:0.31007\ttrain-logloss:0.04330\n",
      "[338]\teval-logloss:0.31245\ttrain-logloss:0.04311\n",
      "[339]\teval-logloss:0.31142\ttrain-logloss:0.04290\n",
      "[340]\teval-logloss:0.30861\ttrain-logloss:0.04274\n",
      "[341]\teval-logloss:0.30925\ttrain-logloss:0.04262\n",
      "Accuracy on test set: 0.9049895428742157, ROC-AUC score for test set: 0.9497074664177353\n",
      "Accuracy on test set: 0.875, ROC-AUC score for test set: 0.9392361111111112, MCC: 0.75\n"
     ]
    }
   ],
   "source": [
    "param = {'learning_rate': 0.31553117247348733,\n",
    "         'max_delta_step': 1.7726044219753656,\n",
    "         'max_depth': 10,\n",
    "         'min_child_weight': 1.3845040588450772,\n",
    "         'num_rounds': 342.68325188584106,\n",
    "         'reg_alpha': 0.531395259755843,\n",
    "         'reg_lambda': 3.744980563764689,\n",
    "         'weight': 0.26187490421514203}\n",
    "\n",
    "\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "#param[\"tree_method\"] = \"gpu_hist\"\n",
    "#param[\"sampling_method\"] = \"gradient_based\"\n",
    "param['objective'] = 'binary:logistic'\n",
    "\n",
    "weights1 = np.array([param[\"weight\"] if binding == 0 else 1.0 for binding in df_train[\"Binding\"]])\n",
    "weights2 = np.array([param[\"weight\"] if binding == 0 else 1.0 for binding in y_train_Mou])\n",
    "\n",
    "weights = np.concatenate([weights1, weights3])2\n",
    "\n",
    "\n",
    "del param[\"num_rounds\"]\n",
    "del param[\"weight\"]\n",
    "\n",
    "dtrain = xgb.DMatrix(np.array(train_X), weight = weights, label = np.array(train_y),\n",
    "            feature_names= feature_names)\n",
    "dtest = xgb.DMatrix(np.array(test_X), label = np.array(test_y),\n",
    "                    feature_names= feature_names)\n",
    "\n",
    "dtest_new = xgb.DMatrix(np.array(X_test_Mou), label = np.array(y_test_Mou), feature_names= feature_names)\n",
    "\n",
    "evallist = [(dtest_new, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "bst = xgb.train(param,  dtrain, int(num_round),evallist, verbose_eval=1)\n",
    "y_test_pred = np.round(bst.predict(dtest))\n",
    "acc_test = np.mean(y_test_pred == np.array(test_y))\n",
    "roc_auc = roc_auc_score(np.array(test_y), bst.predict(dtest))\n",
    "\n",
    "print(\"Accuracy on test set: %s, ROC-AUC score for test set: %s\"  % (acc_test, roc_auc))\n",
    "\n",
    "\n",
    "y_test_new_pred = np.round(bst.predict(dtest_new))\n",
    "acc_test_new = np.mean(y_test_new_pred == np.array(y_test_Mou))\n",
    "roc_auc_new = roc_auc_score(np.array(y_test_Mou), bst.predict(dtest_new))\n",
    "mcc = matthews_corrcoef(np.array(y_test_Mou), y_test_new_pred)\n",
    "\n",
    "print(\"Accuracy on test set: %s, ROC-AUC score for test set: %s, MCC: %s\"  % (acc_test_new, roc_auc_new, mcc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding no Mou et al. data to the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alexk\\anaconda3\\envs\\predicting_km\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:56: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:54:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\teval-logloss:0.72657\ttrain-logloss:0.60033\n",
      "[1]\teval-logloss:0.72145\ttrain-logloss:0.54160\n",
      "[2]\teval-logloss:0.70856\ttrain-logloss:0.50341\n",
      "[3]\teval-logloss:0.72505\ttrain-logloss:0.47235\n",
      "[4]\teval-logloss:0.74472\ttrain-logloss:0.45163\n",
      "[5]\teval-logloss:0.74446\ttrain-logloss:0.42985\n",
      "[6]\teval-logloss:0.74310\ttrain-logloss:0.41524\n",
      "[7]\teval-logloss:0.78860\ttrain-logloss:0.39985\n",
      "[8]\teval-logloss:0.78448\ttrain-logloss:0.38306\n",
      "[9]\teval-logloss:0.81698\ttrain-logloss:0.37547\n",
      "[10]\teval-logloss:0.80540\ttrain-logloss:0.36375\n",
      "[11]\teval-logloss:0.80235\ttrain-logloss:0.35612\n",
      "[12]\teval-logloss:0.79059\ttrain-logloss:0.34820\n",
      "[13]\teval-logloss:0.78559\ttrain-logloss:0.34451\n",
      "[14]\teval-logloss:0.79603\ttrain-logloss:0.33858\n",
      "[15]\teval-logloss:0.82471\ttrain-logloss:0.33341\n",
      "[16]\teval-logloss:0.89267\ttrain-logloss:0.32915\n",
      "[17]\teval-logloss:0.91536\ttrain-logloss:0.32404\n",
      "[18]\teval-logloss:0.90621\ttrain-logloss:0.31899\n",
      "[19]\teval-logloss:0.90562\ttrain-logloss:0.31530\n",
      "[20]\teval-logloss:0.89434\ttrain-logloss:0.31040\n",
      "[21]\teval-logloss:0.89153\ttrain-logloss:0.30623\n",
      "[22]\teval-logloss:0.84976\ttrain-logloss:0.30260\n",
      "[23]\teval-logloss:0.84935\ttrain-logloss:0.29972\n",
      "[24]\teval-logloss:0.86110\ttrain-logloss:0.29291\n",
      "[25]\teval-logloss:0.85719\ttrain-logloss:0.28866\n",
      "[26]\teval-logloss:0.84511\ttrain-logloss:0.28537\n",
      "[27]\teval-logloss:0.84453\ttrain-logloss:0.28220\n",
      "[28]\teval-logloss:0.84444\ttrain-logloss:0.27736\n",
      "[29]\teval-logloss:0.84641\ttrain-logloss:0.27228\n",
      "[30]\teval-logloss:0.86366\ttrain-logloss:0.26713\n",
      "[31]\teval-logloss:0.86377\ttrain-logloss:0.26330\n",
      "[32]\teval-logloss:0.88848\ttrain-logloss:0.26050\n",
      "[33]\teval-logloss:0.88824\ttrain-logloss:0.25871\n",
      "[34]\teval-logloss:0.88726\ttrain-logloss:0.25525\n",
      "[35]\teval-logloss:0.89637\ttrain-logloss:0.25265\n",
      "[36]\teval-logloss:0.89468\ttrain-logloss:0.24992\n",
      "[37]\teval-logloss:0.89450\ttrain-logloss:0.24755\n",
      "[38]\teval-logloss:0.92068\ttrain-logloss:0.24226\n",
      "[39]\teval-logloss:0.93103\ttrain-logloss:0.23992\n",
      "[40]\teval-logloss:0.93019\ttrain-logloss:0.23745\n",
      "[41]\teval-logloss:0.92598\ttrain-logloss:0.23535\n",
      "[42]\teval-logloss:0.94835\ttrain-logloss:0.23269\n",
      "[43]\teval-logloss:0.95991\ttrain-logloss:0.22857\n",
      "[44]\teval-logloss:0.96160\ttrain-logloss:0.22639\n",
      "[45]\teval-logloss:0.97940\ttrain-logloss:0.22411\n",
      "[46]\teval-logloss:0.97822\ttrain-logloss:0.22166\n",
      "[47]\teval-logloss:0.97690\ttrain-logloss:0.21981\n",
      "[48]\teval-logloss:0.98657\ttrain-logloss:0.21840\n",
      "[49]\teval-logloss:0.98676\ttrain-logloss:0.21579\n",
      "[50]\teval-logloss:1.03644\ttrain-logloss:0.21468\n",
      "[51]\teval-logloss:1.03598\ttrain-logloss:0.21270\n",
      "[52]\teval-logloss:1.04794\ttrain-logloss:0.20988\n",
      "[53]\teval-logloss:1.04742\ttrain-logloss:0.20684\n",
      "[54]\teval-logloss:1.05542\ttrain-logloss:0.20508\n",
      "[55]\teval-logloss:1.05542\ttrain-logloss:0.20331\n",
      "[56]\teval-logloss:1.06635\ttrain-logloss:0.20078\n",
      "[57]\teval-logloss:1.10135\ttrain-logloss:0.19929\n",
      "[58]\teval-logloss:1.10117\ttrain-logloss:0.19782\n",
      "[59]\teval-logloss:1.10125\ttrain-logloss:0.19658\n",
      "[60]\teval-logloss:1.11193\ttrain-logloss:0.19496\n",
      "[61]\teval-logloss:1.10783\ttrain-logloss:0.19212\n",
      "[62]\teval-logloss:1.10741\ttrain-logloss:0.19044\n",
      "[63]\teval-logloss:1.10887\ttrain-logloss:0.18934\n",
      "[64]\teval-logloss:1.12824\ttrain-logloss:0.18661\n",
      "[65]\teval-logloss:1.15127\ttrain-logloss:0.18516\n",
      "[66]\teval-logloss:1.15066\ttrain-logloss:0.18378\n",
      "[67]\teval-logloss:1.15004\ttrain-logloss:0.18261\n",
      "[68]\teval-logloss:1.15542\ttrain-logloss:0.18176\n",
      "[69]\teval-logloss:1.15598\ttrain-logloss:0.18028\n",
      "[70]\teval-logloss:1.19057\ttrain-logloss:0.17838\n",
      "[71]\teval-logloss:1.14468\ttrain-logloss:0.17714\n",
      "[72]\teval-logloss:1.14513\ttrain-logloss:0.17619\n",
      "[73]\teval-logloss:1.14025\ttrain-logloss:0.17490\n",
      "[74]\teval-logloss:1.13754\ttrain-logloss:0.17324\n",
      "[75]\teval-logloss:1.14198\ttrain-logloss:0.17212\n",
      "[76]\teval-logloss:1.16437\ttrain-logloss:0.17046\n",
      "[77]\teval-logloss:1.14683\ttrain-logloss:0.16877\n",
      "[78]\teval-logloss:1.14511\ttrain-logloss:0.16742\n",
      "[79]\teval-logloss:1.16157\ttrain-logloss:0.16588\n",
      "[80]\teval-logloss:1.16102\ttrain-logloss:0.16440\n",
      "[81]\teval-logloss:1.15664\ttrain-logloss:0.16268\n",
      "[82]\teval-logloss:1.15265\ttrain-logloss:0.16133\n",
      "[83]\teval-logloss:1.15338\ttrain-logloss:0.15971\n",
      "[84]\teval-logloss:1.15570\ttrain-logloss:0.15827\n",
      "[85]\teval-logloss:1.16637\ttrain-logloss:0.15701\n",
      "[86]\teval-logloss:1.16627\ttrain-logloss:0.15558\n",
      "[87]\teval-logloss:1.18545\ttrain-logloss:0.15438\n",
      "[88]\teval-logloss:1.18758\ttrain-logloss:0.15330\n",
      "[89]\teval-logloss:1.19563\ttrain-logloss:0.15248\n",
      "[90]\teval-logloss:1.22351\ttrain-logloss:0.15096\n",
      "[91]\teval-logloss:1.28679\ttrain-logloss:0.14960\n",
      "[92]\teval-logloss:1.29149\ttrain-logloss:0.14858\n",
      "[93]\teval-logloss:1.30202\ttrain-logloss:0.14775\n",
      "[94]\teval-logloss:1.29704\ttrain-logloss:0.14680\n",
      "[95]\teval-logloss:1.30376\ttrain-logloss:0.14542\n",
      "[96]\teval-logloss:1.30167\ttrain-logloss:0.14445\n",
      "[97]\teval-logloss:1.30250\ttrain-logloss:0.14329\n",
      "[98]\teval-logloss:1.32769\ttrain-logloss:0.14277\n",
      "[99]\teval-logloss:1.32895\ttrain-logloss:0.14205\n",
      "[100]\teval-logloss:1.33170\ttrain-logloss:0.14094\n",
      "[101]\teval-logloss:1.33376\ttrain-logloss:0.13996\n",
      "[102]\teval-logloss:1.33406\ttrain-logloss:0.13912\n",
      "[103]\teval-logloss:1.34429\ttrain-logloss:0.13802\n",
      "[104]\teval-logloss:1.33000\ttrain-logloss:0.13722\n",
      "[105]\teval-logloss:1.34444\ttrain-logloss:0.13631\n",
      "[106]\teval-logloss:1.34483\ttrain-logloss:0.13566\n",
      "[107]\teval-logloss:1.34688\ttrain-logloss:0.13504\n",
      "[108]\teval-logloss:1.36666\ttrain-logloss:0.13411\n",
      "[109]\teval-logloss:1.36930\ttrain-logloss:0.13338\n",
      "[110]\teval-logloss:1.37926\ttrain-logloss:0.13231\n",
      "[111]\teval-logloss:1.39719\ttrain-logloss:0.13138\n",
      "[112]\teval-logloss:1.39363\ttrain-logloss:0.13069\n",
      "[113]\teval-logloss:1.40564\ttrain-logloss:0.12933\n",
      "[114]\teval-logloss:1.41518\ttrain-logloss:0.12840\n",
      "[115]\teval-logloss:1.41278\ttrain-logloss:0.12726\n",
      "[116]\teval-logloss:1.42356\ttrain-logloss:0.12633\n",
      "[117]\teval-logloss:1.42351\ttrain-logloss:0.12534\n",
      "[118]\teval-logloss:1.42732\ttrain-logloss:0.12458\n",
      "[119]\teval-logloss:1.42664\ttrain-logloss:0.12399\n",
      "[120]\teval-logloss:1.42694\ttrain-logloss:0.12352\n",
      "[121]\teval-logloss:1.40865\ttrain-logloss:0.12272\n",
      "[122]\teval-logloss:1.40888\ttrain-logloss:0.12228\n",
      "[123]\teval-logloss:1.40857\ttrain-logloss:0.12170\n",
      "[124]\teval-logloss:1.40764\ttrain-logloss:0.12107\n",
      "[125]\teval-logloss:1.40824\ttrain-logloss:0.12014\n",
      "[126]\teval-logloss:1.40838\ttrain-logloss:0.11910\n",
      "[127]\teval-logloss:1.40231\ttrain-logloss:0.11864\n",
      "[128]\teval-logloss:1.38062\ttrain-logloss:0.11806\n",
      "[129]\teval-logloss:1.38946\ttrain-logloss:0.11711\n",
      "[130]\teval-logloss:1.40002\ttrain-logloss:0.11644\n",
      "[131]\teval-logloss:1.39808\ttrain-logloss:0.11583\n",
      "[132]\teval-logloss:1.39613\ttrain-logloss:0.11506\n",
      "[133]\teval-logloss:1.40616\ttrain-logloss:0.11460\n",
      "[134]\teval-logloss:1.40685\ttrain-logloss:0.11399\n",
      "[135]\teval-logloss:1.40718\ttrain-logloss:0.11349\n",
      "[136]\teval-logloss:1.41237\ttrain-logloss:0.11282\n",
      "[137]\teval-logloss:1.41423\ttrain-logloss:0.11210\n",
      "[138]\teval-logloss:1.40957\ttrain-logloss:0.11086\n",
      "[139]\teval-logloss:1.41084\ttrain-logloss:0.11024\n",
      "[140]\teval-logloss:1.41263\ttrain-logloss:0.10956\n",
      "[141]\teval-logloss:1.41857\ttrain-logloss:0.10906\n",
      "[142]\teval-logloss:1.43215\ttrain-logloss:0.10845\n",
      "[143]\teval-logloss:1.46835\ttrain-logloss:0.10769\n",
      "[144]\teval-logloss:1.46848\ttrain-logloss:0.10706\n",
      "[145]\teval-logloss:1.46990\ttrain-logloss:0.10627\n",
      "[146]\teval-logloss:1.43720\ttrain-logloss:0.10559\n",
      "[147]\teval-logloss:1.43583\ttrain-logloss:0.10518\n",
      "[148]\teval-logloss:1.43243\ttrain-logloss:0.10421\n",
      "[149]\teval-logloss:1.43311\ttrain-logloss:0.10380\n",
      "[150]\teval-logloss:1.43428\ttrain-logloss:0.10310\n",
      "[151]\teval-logloss:1.44633\ttrain-logloss:0.10247\n",
      "[152]\teval-logloss:1.44640\ttrain-logloss:0.10175\n",
      "[153]\teval-logloss:1.46122\ttrain-logloss:0.10118\n",
      "[154]\teval-logloss:1.44681\ttrain-logloss:0.10011\n",
      "[155]\teval-logloss:1.44555\ttrain-logloss:0.09931\n",
      "[156]\teval-logloss:1.44516\ttrain-logloss:0.09866\n",
      "[157]\teval-logloss:1.44169\ttrain-logloss:0.09788\n",
      "[158]\teval-logloss:1.44588\ttrain-logloss:0.09744\n",
      "[159]\teval-logloss:1.44596\ttrain-logloss:0.09683\n",
      "[160]\teval-logloss:1.44722\ttrain-logloss:0.09635\n",
      "[161]\teval-logloss:1.44594\ttrain-logloss:0.09599\n",
      "[162]\teval-logloss:1.45087\ttrain-logloss:0.09563\n",
      "[163]\teval-logloss:1.45677\ttrain-logloss:0.09505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[164]\teval-logloss:1.46034\ttrain-logloss:0.09464\n",
      "[165]\teval-logloss:1.46955\ttrain-logloss:0.09414\n",
      "[166]\teval-logloss:1.48177\ttrain-logloss:0.09364\n",
      "[167]\teval-logloss:1.48618\ttrain-logloss:0.09301\n",
      "[168]\teval-logloss:1.48571\ttrain-logloss:0.09244\n",
      "[169]\teval-logloss:1.49730\ttrain-logloss:0.09177\n",
      "[170]\teval-logloss:1.49102\ttrain-logloss:0.09139\n",
      "[171]\teval-logloss:1.47989\ttrain-logloss:0.09072\n",
      "[172]\teval-logloss:1.48629\ttrain-logloss:0.09010\n",
      "[173]\teval-logloss:1.49598\ttrain-logloss:0.08942\n",
      "[174]\teval-logloss:1.50819\ttrain-logloss:0.08903\n",
      "[175]\teval-logloss:1.49070\ttrain-logloss:0.08856\n",
      "[176]\teval-logloss:1.49049\ttrain-logloss:0.08816\n",
      "[177]\teval-logloss:1.49424\ttrain-logloss:0.08751\n",
      "[178]\teval-logloss:1.52745\ttrain-logloss:0.08679\n",
      "[179]\teval-logloss:1.52871\ttrain-logloss:0.08637\n",
      "[180]\teval-logloss:1.52662\ttrain-logloss:0.08547\n",
      "[181]\teval-logloss:1.53573\ttrain-logloss:0.08508\n",
      "[182]\teval-logloss:1.53593\ttrain-logloss:0.08477\n",
      "[183]\teval-logloss:1.55252\ttrain-logloss:0.08441\n",
      "[184]\teval-logloss:1.55555\ttrain-logloss:0.08404\n",
      "[185]\teval-logloss:1.55575\ttrain-logloss:0.08371\n",
      "[186]\teval-logloss:1.57013\ttrain-logloss:0.08328\n",
      "[187]\teval-logloss:1.57119\ttrain-logloss:0.08286\n",
      "[188]\teval-logloss:1.55730\ttrain-logloss:0.08238\n",
      "[189]\teval-logloss:1.59719\ttrain-logloss:0.08144\n",
      "[190]\teval-logloss:1.59571\ttrain-logloss:0.08086\n",
      "[191]\teval-logloss:1.59711\ttrain-logloss:0.08044\n",
      "[192]\teval-logloss:1.59997\ttrain-logloss:0.08008\n",
      "[193]\teval-logloss:1.60091\ttrain-logloss:0.07984\n",
      "[194]\teval-logloss:1.60089\ttrain-logloss:0.07928\n",
      "[195]\teval-logloss:1.60143\ttrain-logloss:0.07909\n",
      "[196]\teval-logloss:1.59970\ttrain-logloss:0.07873\n",
      "[197]\teval-logloss:1.60139\ttrain-logloss:0.07820\n",
      "[198]\teval-logloss:1.58942\ttrain-logloss:0.07755\n",
      "[199]\teval-logloss:1.59626\ttrain-logloss:0.07714\n",
      "[200]\teval-logloss:1.60597\ttrain-logloss:0.07693\n",
      "[201]\teval-logloss:1.57897\ttrain-logloss:0.07653\n",
      "[202]\teval-logloss:1.57795\ttrain-logloss:0.07629\n",
      "[203]\teval-logloss:1.57407\ttrain-logloss:0.07586\n",
      "[204]\teval-logloss:1.57389\ttrain-logloss:0.07552\n",
      "[205]\teval-logloss:1.55409\ttrain-logloss:0.07503\n",
      "[206]\teval-logloss:1.55225\ttrain-logloss:0.07458\n",
      "[207]\teval-logloss:1.54973\ttrain-logloss:0.07416\n",
      "[208]\teval-logloss:1.55008\ttrain-logloss:0.07382\n",
      "[209]\teval-logloss:1.56233\ttrain-logloss:0.07354\n",
      "[210]\teval-logloss:1.57931\ttrain-logloss:0.07308\n",
      "[211]\teval-logloss:1.58358\ttrain-logloss:0.07251\n",
      "[212]\teval-logloss:1.59148\ttrain-logloss:0.07219\n",
      "[213]\teval-logloss:1.59276\ttrain-logloss:0.07190\n",
      "[214]\teval-logloss:1.60417\ttrain-logloss:0.07173\n",
      "[215]\teval-logloss:1.60402\ttrain-logloss:0.07137\n",
      "[216]\teval-logloss:1.60708\ttrain-logloss:0.07103\n",
      "[217]\teval-logloss:1.60888\ttrain-logloss:0.07067\n",
      "[218]\teval-logloss:1.60895\ttrain-logloss:0.07045\n",
      "[219]\teval-logloss:1.58820\ttrain-logloss:0.07021\n",
      "[220]\teval-logloss:1.58757\ttrain-logloss:0.06986\n",
      "[221]\teval-logloss:1.58725\ttrain-logloss:0.06964\n",
      "[222]\teval-logloss:1.58633\ttrain-logloss:0.06945\n",
      "[223]\teval-logloss:1.58661\ttrain-logloss:0.06917\n",
      "[224]\teval-logloss:1.61809\ttrain-logloss:0.06897\n",
      "[225]\teval-logloss:1.62120\ttrain-logloss:0.06859\n",
      "[226]\teval-logloss:1.60594\ttrain-logloss:0.06842\n",
      "[227]\teval-logloss:1.60589\ttrain-logloss:0.06819\n",
      "[228]\teval-logloss:1.60694\ttrain-logloss:0.06799\n",
      "[229]\teval-logloss:1.61466\ttrain-logloss:0.06772\n",
      "[230]\teval-logloss:1.61534\ttrain-logloss:0.06748\n",
      "[231]\teval-logloss:1.61621\ttrain-logloss:0.06735\n",
      "[232]\teval-logloss:1.61630\ttrain-logloss:0.06702\n",
      "[233]\teval-logloss:1.63701\ttrain-logloss:0.06665\n",
      "[234]\teval-logloss:1.63441\ttrain-logloss:0.06633\n",
      "[235]\teval-logloss:1.63507\ttrain-logloss:0.06590\n",
      "[236]\teval-logloss:1.63035\ttrain-logloss:0.06554\n",
      "[237]\teval-logloss:1.63051\ttrain-logloss:0.06527\n",
      "[238]\teval-logloss:1.63131\ttrain-logloss:0.06502\n",
      "[239]\teval-logloss:1.63685\ttrain-logloss:0.06476\n",
      "[240]\teval-logloss:1.63522\ttrain-logloss:0.06447\n",
      "[241]\teval-logloss:1.63336\ttrain-logloss:0.06422\n",
      "[242]\teval-logloss:1.63369\ttrain-logloss:0.06386\n",
      "[243]\teval-logloss:1.63551\ttrain-logloss:0.06347\n",
      "[244]\teval-logloss:1.63531\ttrain-logloss:0.06322\n",
      "[245]\teval-logloss:1.64095\ttrain-logloss:0.06270\n",
      "[246]\teval-logloss:1.64024\ttrain-logloss:0.06239\n",
      "[247]\teval-logloss:1.63817\ttrain-logloss:0.06215\n",
      "[248]\teval-logloss:1.63852\ttrain-logloss:0.06187\n",
      "[249]\teval-logloss:1.63928\ttrain-logloss:0.06159\n",
      "[250]\teval-logloss:1.63182\ttrain-logloss:0.06143\n",
      "[251]\teval-logloss:1.63186\ttrain-logloss:0.06116\n",
      "[252]\teval-logloss:1.63696\ttrain-logloss:0.06089\n",
      "[253]\teval-logloss:1.64609\ttrain-logloss:0.06060\n",
      "[254]\teval-logloss:1.66008\ttrain-logloss:0.06031\n",
      "[255]\teval-logloss:1.69398\ttrain-logloss:0.05988\n",
      "[256]\teval-logloss:1.69398\ttrain-logloss:0.05965\n",
      "[257]\teval-logloss:1.69266\ttrain-logloss:0.05944\n",
      "[258]\teval-logloss:1.69167\ttrain-logloss:0.05928\n",
      "[259]\teval-logloss:1.69286\ttrain-logloss:0.05908\n",
      "[260]\teval-logloss:1.69176\ttrain-logloss:0.05891\n",
      "[261]\teval-logloss:1.69285\ttrain-logloss:0.05876\n",
      "[262]\teval-logloss:1.69402\ttrain-logloss:0.05855\n",
      "[263]\teval-logloss:1.69277\ttrain-logloss:0.05829\n",
      "[264]\teval-logloss:1.69822\ttrain-logloss:0.05794\n",
      "[265]\teval-logloss:1.69784\ttrain-logloss:0.05766\n",
      "[266]\teval-logloss:1.70456\ttrain-logloss:0.05749\n",
      "[267]\teval-logloss:1.68778\ttrain-logloss:0.05721\n",
      "[268]\teval-logloss:1.68711\ttrain-logloss:0.05702\n",
      "[269]\teval-logloss:1.68944\ttrain-logloss:0.05683\n",
      "[270]\teval-logloss:1.69741\ttrain-logloss:0.05653\n",
      "[271]\teval-logloss:1.72678\ttrain-logloss:0.05628\n",
      "[272]\teval-logloss:1.71731\ttrain-logloss:0.05608\n",
      "[273]\teval-logloss:1.71744\ttrain-logloss:0.05585\n",
      "[274]\teval-logloss:1.71744\ttrain-logloss:0.05568\n",
      "[275]\teval-logloss:1.71811\ttrain-logloss:0.05555\n",
      "[276]\teval-logloss:1.71088\ttrain-logloss:0.05538\n",
      "[277]\teval-logloss:1.71285\ttrain-logloss:0.05518\n",
      "[278]\teval-logloss:1.67764\ttrain-logloss:0.05496\n",
      "[279]\teval-logloss:1.64619\ttrain-logloss:0.05477\n",
      "[280]\teval-logloss:1.61696\ttrain-logloss:0.05453\n",
      "[281]\teval-logloss:1.62281\ttrain-logloss:0.05417\n",
      "[282]\teval-logloss:1.62313\ttrain-logloss:0.05391\n",
      "[283]\teval-logloss:1.62313\ttrain-logloss:0.05373\n",
      "[284]\teval-logloss:1.62464\ttrain-logloss:0.05353\n",
      "[285]\teval-logloss:1.62211\ttrain-logloss:0.05339\n",
      "[286]\teval-logloss:1.62410\ttrain-logloss:0.05313\n",
      "[287]\teval-logloss:1.63129\ttrain-logloss:0.05293\n",
      "[288]\teval-logloss:1.63129\ttrain-logloss:0.05271\n",
      "[289]\teval-logloss:1.62669\ttrain-logloss:0.05241\n",
      "[290]\teval-logloss:1.63495\ttrain-logloss:0.05216\n",
      "[291]\teval-logloss:1.61959\ttrain-logloss:0.05193\n",
      "[292]\teval-logloss:1.61922\ttrain-logloss:0.05172\n",
      "[293]\teval-logloss:1.61543\ttrain-logloss:0.05153\n",
      "[294]\teval-logloss:1.61837\ttrain-logloss:0.05124\n",
      "[295]\teval-logloss:1.61837\ttrain-logloss:0.05112\n",
      "[296]\teval-logloss:1.61817\ttrain-logloss:0.05096\n",
      "[297]\teval-logloss:1.61821\ttrain-logloss:0.05082\n",
      "[298]\teval-logloss:1.61477\ttrain-logloss:0.05057\n",
      "[299]\teval-logloss:1.61457\ttrain-logloss:0.05035\n",
      "[300]\teval-logloss:1.61822\ttrain-logloss:0.05018\n",
      "[301]\teval-logloss:1.61949\ttrain-logloss:0.04995\n",
      "[302]\teval-logloss:1.62031\ttrain-logloss:0.04968\n",
      "[303]\teval-logloss:1.62551\ttrain-logloss:0.04948\n",
      "[304]\teval-logloss:1.63837\ttrain-logloss:0.04932\n",
      "[305]\teval-logloss:1.64246\ttrain-logloss:0.04910\n",
      "[306]\teval-logloss:1.64246\ttrain-logloss:0.04889\n",
      "[307]\teval-logloss:1.61489\ttrain-logloss:0.04863\n",
      "[308]\teval-logloss:1.62105\ttrain-logloss:0.04845\n",
      "[309]\teval-logloss:1.62078\ttrain-logloss:0.04830\n",
      "[310]\teval-logloss:1.61488\ttrain-logloss:0.04817\n",
      "[311]\teval-logloss:1.61141\ttrain-logloss:0.04803\n",
      "[312]\teval-logloss:1.60999\ttrain-logloss:0.04786\n",
      "[313]\teval-logloss:1.62053\ttrain-logloss:0.04768\n",
      "[314]\teval-logloss:1.60554\ttrain-logloss:0.04757\n",
      "[315]\teval-logloss:1.60874\ttrain-logloss:0.04730\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train = pd.read_pickle(join(CURRENT_DIR, \"..\" ,\"data\",\n",
    "                               \"splits\", \"df_train_with_ESM1b_ts.pkl\"))\n",
    "df_train = df_train.loc[df_train[\"ESM1b_ts\"] != \"\"]\n",
    "df_train.reset_index(inplace = True, drop = True)\n",
    "\n",
    "\n",
    "train_X, train_y =  create_input_and_output_data(df = df_train)\n",
    "test_X, test_y =  create_input_and_output_data(df = df_test)\n",
    "\n",
    "df_test_new = df_Mou.copy()\n",
    "df_test_new[\"Binding\"] = [y > 2 for y in df_test_new[\"activity\"]]\n",
    "test_new_X, test_new_y =  create_input_and_output_data(df = df_test_new)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#same split as in Mou et al paper:\n",
    "X_train_Mou, X_test_Mou, y_train_Mou, y_test_Mou = train_test_split(test_new_X, test_new_y,\n",
    "                                                                    test_size = 0.20, random_state = 888)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "param = {'learning_rate': 0.31553117247348733,\n",
    "         'max_delta_step': 1.7726044219753656,\n",
    "         'max_depth': 10,\n",
    "         'min_child_weight': 1.3845040588450772,\n",
    "         'num_rounds': 342.68325188584106,\n",
    "         'reg_alpha': 0.531395259755843,\n",
    "         'reg_lambda': 3.744980563764689,\n",
    "         'weight': 0.26187490421514203}\n",
    "\n",
    "\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "#param[\"tree_method\"] = \"gpu_hist\"\n",
    "#param[\"sampling_method\"] = \"gradient_based\"\n",
    "param['objective'] = 'binary:logistic'\n",
    "\n",
    "weights =  np.array([param[\"weight\"] if binding == 0 else 1.0 for binding in np.array(train_y)])\n",
    "\n",
    "del param[\"num_rounds\"]\n",
    "del param[\"weight\"]\n",
    "\n",
    "dtrain = xgb.DMatrix(np.array(train_X), weight = weights, label = np.array(train_y),\n",
    "            feature_names= feature_names)\n",
    "dtest = xgb.DMatrix(np.array(test_X), label = np.array(test_y),\n",
    "                    feature_names= feature_names)\n",
    "\n",
    "dtest_new = xgb.DMatrix(np.array(X_test_Mou), label = np.array(y_test_Mou), feature_names= feature_names)\n",
    "\n",
    "evallist = [(dtest_new, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "bst = xgb.train(param,  dtrain, int(num_round),evallist, verbose_eval=1)\n",
    "y_test_pred = np.round(bst.predict(dtest))\n",
    "acc_test = np.mean(y_test_pred == np.array(test_y))\n",
    "roc_auc = roc_auc_score(np.array(test_y), bst.predict(dtest))\n",
    "\n",
    "print(\"Accuracy on test set: %s, ROC-AUC score for test set: %s\"  % (acc_test, roc_auc))\n",
    "\n",
    "\n",
    "y_test_new_pred = np.round(bst.predict(dtest_new))\n",
    "acc_test_new = np.mean(y_test_new_pred == np.array(y_test_Mou))\n",
    "roc_auc_new = roc_auc_score(np.array(y_test_Mou), bst.predict(dtest_new))\n",
    "mcc = matthews_corrcoef(np.array(y_test_Mou), y_test_new_pred)\n",
    "\n",
    "print(\"Accuracy on test set: %s, ROC-AUC score for test set: %s, MCC: %s\"  % (acc_test_new, roc_auc_new, mcc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
