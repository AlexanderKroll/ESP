{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training gradient boosting model for enzyme-substrate pair prediction with ESM-1b-vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and preprocessing data for model training and evaluation\n",
    "### 2. Hyperparameter optimization using a 5-fold cross-validation (CV)\n",
    "### 3. Training and validating the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexk\\projects\\SubFinder\\notebooks_and_code\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from os.path import join\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, rand\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append('.\\\\additional_code')\n",
    "#from data_preprocessing import *\n",
    "\n",
    "CURRENT_DIR = os.getcwd()\n",
    "print(CURRENT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and preprocessing data for model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_column_to_strings(df, column):\n",
    "    df[column] = [str(list(df[column][ind])) for ind in df.index]\n",
    "    return(df)\n",
    "\n",
    "def string_column_to_array(df, column):\n",
    "    df[column] = [np.array(eval(df[column][ind])) for ind in df.index]\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Loading data: \n",
    "Only keeping data points from the GO Annotation database with experimental evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexk\\anaconda3\\envs\\Predicting_Km\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:56: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n",
      "C:\\Users\\alexk\\anaconda3\\envs\\Predicting_Km\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:56: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_pickle(join(CURRENT_DIR, \"..\" ,\"data\",\"splits\", \"df_train_with_ESM1b_ts_promiscous.pkl\"))\n",
    "df_train = df_train.loc[df_train[\"ESM1b\"] != \"\"]\n",
    "df_train = df_train.loc[df_train[\"type\"] != \"engqvist\"]\n",
    "df_train.reset_index(inplace = True, drop = True)\n",
    "\n",
    "df_test  = pd.read_pickle(join(CURRENT_DIR, \"..\" ,\"data\",\"splits\", \"df_test_with_ESM1b_ts_promiscous.pkl\"))\n",
    "df_test = df_test.loc[df_test[\"ESM1b\"] != \"\"]\n",
    "df_test = df_test.loc[df_test[\"type\"] != \"engqvist\"]\n",
    "df_test.reset_index(inplace = True, drop = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Splitting training set into 5-folds for hyperparameter optimization:\n",
    "The 5 folds are created in such a way that the same enzyme does not occure in two different folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, frac):\n",
    "    df1 = pd.DataFrame(columns = list(df.columns))\n",
    "    df2 = pd.DataFrame(columns = list(df.columns))\n",
    "    try:\n",
    "        df.drop(columns = [\"level_0\"], inplace = True)\n",
    "    except: \n",
    "        pass\n",
    "    df.reset_index(inplace = True)\n",
    "    \n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    ind = 0\n",
    "    while len(train_indices) +len(test_indices) < len(df):\n",
    "        if ind not in train_indices and ind not in test_indices:\n",
    "            if ind % frac != 0:\n",
    "                n_old = len(train_indices)\n",
    "                train_indices.append(ind)\n",
    "                train_indices = list(set(train_indices))\n",
    "\n",
    "                while n_old != len(train_indices):\n",
    "                    n_old = len(train_indices)\n",
    "\n",
    "                    training_seqs= list(set(df[\"ESM1b\"].loc[train_indices]))\n",
    "\n",
    "                    train_indices = train_indices + (list(df.loc[df[\"ESM1b\"].isin(training_seqs)].index))\n",
    "                    train_indices = list(set(train_indices))\n",
    "                \n",
    "            else:\n",
    "                n_old = len(test_indices)\n",
    "                test_indices.append(ind)\n",
    "                test_indices = list(set(test_indices))\n",
    "\n",
    "                while n_old != len(test_indices):\n",
    "                    n_old = len(test_indices)\n",
    "\n",
    "                    testing_seqs= list(set(df[\"ESM1b\"].loc[test_indices]))\n",
    "\n",
    "                    test_indices = test_indices + (list(df.loc[df[\"ESM1b\"].isin(testing_seqs)].index))\n",
    "                    test_indices = list(set(test_indices))\n",
    "                \n",
    "        ind +=1\n",
    "    return(df.loc[train_indices], df.loc[test_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8933 2183\n",
      "6793 2140\n",
      "4579 2214\n",
      "2025 2554\n"
     ]
    }
   ],
   "source": [
    "data_train2 = df_train.copy()\n",
    "data_train2 = array_column_to_strings(data_train2, column = \"ESM1b\")\n",
    "\n",
    "data_train2, df_fold = split_dataframe(df = data_train2, frac=5)\n",
    "indices_fold1 = list(df_fold[\"index\"])\n",
    "print(len(data_train2), len(indices_fold1))#\n",
    "\n",
    "data_train2, df_fold = split_dataframe(df = data_train2, frac=4)\n",
    "indices_fold2 = list(df_fold[\"index\"])\n",
    "print(len(data_train2), len(indices_fold2))\n",
    "\n",
    "data_train2, df_fold = split_dataframe(df = data_train2, frac=3)\n",
    "indices_fold3 = list(df_fold[\"index\"])\n",
    "print(len(data_train2), len(indices_fold3))\n",
    "\n",
    "data_train2, df_fold = split_dataframe(df = data_train2, frac=2)\n",
    "indices_fold4 = list(df_fold[\"index\"])\n",
    "indices_fold5 = list(data_train2[\"index\"])\n",
    "print(len(data_train2), len(indices_fold4))\n",
    "\n",
    "\n",
    "fold_indices = [indices_fold1, indices_fold2, indices_fold3, indices_fold4, indices_fold5]\n",
    "\n",
    "train_indices = [[], [], [], [], []]\n",
    "test_indices = [[], [], [], [], []]\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if i != j:\n",
    "            train_indices[i] = train_indices[i] + fold_indices[j]\n",
    "            \n",
    "    test_indices[i] = fold_indices[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter optimization using a 5-fold cross-validation (CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) ECFP and ESM1b:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Creating numpy arrays with input vectors and output variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_and_output_data(df):\n",
    "    X = ();\n",
    "    y = ();\n",
    "    \n",
    "    for ind in df.index:\n",
    "        emb = df[\"ESM1b_ts\"][ind]\n",
    "        ecfp = np.array(list(df[\"ECFP\"][ind])).astype(int)\n",
    "                \n",
    "        X = X +(np.concatenate([ecfp, emb]), );\n",
    "        y = y + (df[\"Binding\"][ind], );\n",
    "\n",
    "    return(X,y)\n",
    "\n",
    "train_X, train_y =  create_input_and_output_data(df = df_train)\n",
    "test_X, test_y =  create_input_and_output_data(df = df_test)\n",
    "\n",
    "\n",
    "feature_names =  [\"ECFP_\" + str(i) for i in range(1024)]\n",
    "feature_names = feature_names + [\"ESM1b_\" + str(i) for i in range(1280)]\n",
    "\n",
    "train_X = np.array(train_X)\n",
    "test_X  = np.array(test_X)\n",
    "\n",
    "train_y = np.array(train_y)\n",
    "test_y  = np.array(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Performing hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_neg_acc_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    weights = np.array([param[\"weight\"] if binding == 0 else 1.0 for binding in df_train[\"Binding\"]])\n",
    "    \n",
    "    del param[\"num_rounds\"]\n",
    "    del param[\"weight\"]\n",
    "    \n",
    "    loss = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(np.array(train_X[train_index]), weight = weights[train_index],\n",
    "                         label = np.array(train_y[train_index]))\n",
    "        dvalid = xgb.DMatrix(np.array(train_X[test_index]))\n",
    "        bst = xgb.train(param,  dtrain, int(num_round), verbose_eval=1)\n",
    "        y_valid_pred = np.round(bst.predict(dvalid))\n",
    "        validation_y = train_y[test_index]\n",
    "    \n",
    "        false_positive = 100*(1-np.mean(np.array(validation_y)[y_valid_pred == 1]))\n",
    "        false_negative = 100*(np.mean(np.array(validation_y)[y_valid_pred == 0]))\n",
    "        logging.info(\"False positive rate: \" + str(false_positive)+ \"; False negative rate: \" + str(false_negative))\n",
    "        loss.append(2*(false_negative**2) + false_positive**1.3)\n",
    "    return(np.mean(loss))\n",
    "\n",
    "#Defining search space for hyperparameter optimization\n",
    "space_gradient_boosting = {\"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.5),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", [9,10,11,12,13]),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 200, 400),\n",
    "    \"weight\" : hp.uniform(\"weight\", 0.1,0.33)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing a random grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 1/1 [04:24<00:00, 264.48s/trial, best loss: 262.5389853662725]\n",
      "1\n",
      "262.5389853662725\n",
      "{'learning_rate': 0.1148489436009765, 'max_delta_step': 3.1684783173308557, 'max_depth': 1, 'min_child_weight': 2.47149091325713, 'num_rounds': 363.7908208797275, 'reg_alpha': 2.8761434380847724, 'reg_lambda': 1.2188050305765386, 'weight': 0.2533132829576781}\n",
      "100%|██████████████████████████████████████████████████| 2/2 [03:15<00:00, 195.00s/trial, best loss: 262.5389853662725]\n",
      "2\n",
      "262.5389853662725\n",
      "{'learning_rate': 0.1148489436009765, 'max_delta_step': 3.1684783173308557, 'max_depth': 1, 'min_child_weight': 2.47149091325713, 'num_rounds': 363.7908208797275, 'reg_alpha': 2.8761434380847724, 'reg_lambda': 1.2188050305765386, 'weight': 0.2533132829576781}\n",
      "100%|███████████████████████████████████████████████████| 3/3 [09:17<00:00, 557.23s/trial, best loss: 255.037971745901]\n",
      "3\n",
      "255.037971745901\n",
      "{'learning_rate': 0.09102301643349951, 'max_delta_step': 0.19773875530069707, 'max_depth': 1, 'min_child_weight': 0.39652446251827267, 'num_rounds': 355.99203554516646, 'reg_alpha': 0.7271180206250938, 'reg_lambda': 2.1749696665365734, 'weight': 0.26478322324359405}\n",
      "100%|███████████████████████████████████████████████████| 4/4 [01:53<00:00, 113.04s/trial, best loss: 255.037971745901]\n",
      "4\n",
      "255.037971745901\n",
      "{'learning_rate': 0.09102301643349951, 'max_delta_step': 0.19773875530069707, 'max_depth': 1, 'min_child_weight': 0.39652446251827267, 'num_rounds': 355.99203554516646, 'reg_alpha': 0.7271180206250938, 'reg_lambda': 2.1749696665365734, 'weight': 0.26478322324359405}\n",
      "100%|████████████████████████████████████████████████████| 5/5 [01:15<00:00, 75.59s/trial, best loss: 255.037971745901]\n",
      "5\n",
      "255.037971745901\n",
      "{'learning_rate': 0.09102301643349951, 'max_delta_step': 0.19773875530069707, 'max_depth': 1, 'min_child_weight': 0.39652446251827267, 'num_rounds': 355.99203554516646, 'reg_alpha': 0.7271180206250938, 'reg_lambda': 2.1749696665365734, 'weight': 0.26478322324359405}\n",
      "100%|████████████████████████████████████████████████████| 6/6 [00:55<00:00, 55.92s/trial, best loss: 255.037971745901]\n",
      "6\n",
      "255.037971745901\n",
      "{'learning_rate': 0.09102301643349951, 'max_delta_step': 0.19773875530069707, 'max_depth': 1, 'min_child_weight': 0.39652446251827267, 'num_rounds': 355.99203554516646, 'reg_alpha': 0.7271180206250938, 'reg_lambda': 2.1749696665365734, 'weight': 0.26478322324359405}\n",
      "100%|███████████████████████████████████████████████████| 7/7 [03:22<00:00, 202.20s/trial, best loss: 255.037971745901]\n",
      "7\n",
      "255.037971745901\n",
      "{'learning_rate': 0.09102301643349951, 'max_delta_step': 0.19773875530069707, 'max_depth': 1, 'min_child_weight': 0.39652446251827267, 'num_rounds': 355.99203554516646, 'reg_alpha': 0.7271180206250938, 'reg_lambda': 2.1749696665365734, 'weight': 0.26478322324359405}\n",
      "100%|████████████████████████████████████████████████████| 8/8 [01:18<00:00, 78.55s/trial, best loss: 255.037971745901]\n",
      "8\n",
      "255.037971745901\n",
      "{'learning_rate': 0.09102301643349951, 'max_delta_step': 0.19773875530069707, 'max_depth': 1, 'min_child_weight': 0.39652446251827267, 'num_rounds': 355.99203554516646, 'reg_alpha': 0.7271180206250938, 'reg_lambda': 2.1749696665365734, 'weight': 0.26478322324359405}\n",
      "100%|██████████████████████████████████████████████████| 9/9 [02:51<00:00, 171.80s/trial, best loss: 253.6826050122041]\n",
      "9\n",
      "253.6826050122041\n",
      "{'learning_rate': 0.17600392450633315, 'max_delta_step': 1.204110529702846, 'max_depth': 1, 'min_child_weight': 3.1896308745823183, 'num_rounds': 298.19866810959155, 'reg_alpha': 0.27065867995679893, 'reg_lambda': 1.3326635539401765, 'weight': 0.16342544218583369}\n",
      "100%|████████████████████████████████████████████████| 10/10 [02:29<00:00, 149.40s/trial, best loss: 253.6826050122041]\n",
      "10\n",
      "253.6826050122041\n",
      "{'learning_rate': 0.17600392450633315, 'max_delta_step': 1.204110529702846, 'max_depth': 1, 'min_child_weight': 3.1896308745823183, 'num_rounds': 298.19866810959155, 'reg_alpha': 0.27065867995679893, 'reg_lambda': 1.3326635539401765, 'weight': 0.16342544218583369}\n",
      "100%|████████████████████████████████████████████████| 11/11 [01:46<00:00, 106.07s/trial, best loss: 253.6826050122041]\n",
      "11\n",
      "253.6826050122041\n",
      "{'learning_rate': 0.17600392450633315, 'max_delta_step': 1.204110529702846, 'max_depth': 1, 'min_child_weight': 3.1896308745823183, 'num_rounds': 298.19866810959155, 'reg_alpha': 0.27065867995679893, 'reg_lambda': 1.3326635539401765, 'weight': 0.16342544218583369}\n",
      "100%|████████████████████████████████████████████████| 12/12 [01:50<00:00, 110.79s/trial, best loss: 253.6826050122041]\n",
      "12\n",
      "253.6826050122041\n",
      "{'learning_rate': 0.17600392450633315, 'max_delta_step': 1.204110529702846, 'max_depth': 1, 'min_child_weight': 3.1896308745823183, 'num_rounds': 298.19866810959155, 'reg_alpha': 0.27065867995679893, 'reg_lambda': 1.3326635539401765, 'weight': 0.16342544218583369}\n",
      "100%|████████████████████████████████████████████████| 13/13 [03:07<00:00, 187.62s/trial, best loss: 253.6826050122041]\n",
      "13\n",
      "253.6826050122041\n",
      "{'learning_rate': 0.17600392450633315, 'max_delta_step': 1.204110529702846, 'max_depth': 1, 'min_child_weight': 3.1896308745823183, 'num_rounds': 298.19866810959155, 'reg_alpha': 0.27065867995679893, 'reg_lambda': 1.3326635539401765, 'weight': 0.16342544218583369}\n",
      "100%|█████████████████████████████████████████████████| 14/14 [01:14<00:00, 74.03s/trial, best loss: 253.6826050122041]\n",
      "14\n",
      "253.6826050122041\n",
      "{'learning_rate': 0.17600392450633315, 'max_delta_step': 1.204110529702846, 'max_depth': 1, 'min_child_weight': 3.1896308745823183, 'num_rounds': 298.19866810959155, 'reg_alpha': 0.27065867995679893, 'reg_lambda': 1.3326635539401765, 'weight': 0.16342544218583369}\n",
      "100%|█████████████████████████████████████████████████| 15/15 [01:16<00:00, 76.70s/trial, best loss: 253.6826050122041]\n",
      "15\n",
      "253.6826050122041\n",
      "{'learning_rate': 0.17600392450633315, 'max_delta_step': 1.204110529702846, 'max_depth': 1, 'min_child_weight': 3.1896308745823183, 'num_rounds': 298.19866810959155, 'reg_alpha': 0.27065867995679893, 'reg_lambda': 1.3326635539401765, 'weight': 0.16342544218583369}\n",
      "100%|█████████████████████████████████████████████████| 16/16 [01:04<00:00, 64.60s/trial, best loss: 253.6826050122041]\n",
      "16\n",
      "253.6826050122041\n",
      "{'learning_rate': 0.17600392450633315, 'max_delta_step': 1.204110529702846, 'max_depth': 1, 'min_child_weight': 3.1896308745823183, 'num_rounds': 298.19866810959155, 'reg_alpha': 0.27065867995679893, 'reg_lambda': 1.3326635539401765, 'weight': 0.16342544218583369}\n",
      "100%|████████████████████████████████████████████████| 17/17 [02:10<00:00, 130.17s/trial, best loss: 253.6826050122041]\n",
      "17\n",
      "253.6826050122041\n",
      "{'learning_rate': 0.17600392450633315, 'max_delta_step': 1.204110529702846, 'max_depth': 1, 'min_child_weight': 3.1896308745823183, 'num_rounds': 298.19866810959155, 'reg_alpha': 0.27065867995679893, 'reg_lambda': 1.3326635539401765, 'weight': 0.16342544218583369}\n",
      "100%|████████████████████████████████████████████████| 18/18 [05:23<00:00, 323.75s/trial, best loss: 253.6826050122041]\n",
      "18\n",
      "253.6826050122041\n",
      "{'learning_rate': 0.17600392450633315, 'max_delta_step': 1.204110529702846, 'max_depth': 1, 'min_child_weight': 3.1896308745823183, 'num_rounds': 298.19866810959155, 'reg_alpha': 0.27065867995679893, 'reg_lambda': 1.3326635539401765, 'weight': 0.16342544218583369}\n",
      "100%|████████████████████████████████████████████████| 19/19 [02:36<00:00, 156.06s/trial, best loss: 253.6826050122041]\n",
      "19\n",
      "253.6826050122041\n",
      "{'learning_rate': 0.17600392450633315, 'max_delta_step': 1.204110529702846, 'max_depth': 1, 'min_child_weight': 3.1896308745823183, 'num_rounds': 298.19866810959155, 'reg_alpha': 0.27065867995679893, 'reg_lambda': 1.3326635539401765, 'weight': 0.16342544218583369}\n",
      "100%|████████████████████████████████████████████████| 20/20 [04:03<00:00, 243.95s/trial, best loss: 253.6826050122041]\n",
      "20\n",
      "253.6826050122041\n",
      "{'learning_rate': 0.17600392450633315, 'max_delta_step': 1.204110529702846, 'max_depth': 1, 'min_child_weight': 3.1896308745823183, 'num_rounds': 298.19866810959155, 'reg_alpha': 0.27065867995679893, 'reg_lambda': 1.3326635539401765, 'weight': 0.16342544218583369}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 21/21 [01:53<00:00, 114.00s/trial, best loss: 253.6826050122041]\n",
      "21\n",
      "253.6826050122041\n",
      "{'learning_rate': 0.17600392450633315, 'max_delta_step': 1.204110529702846, 'max_depth': 1, 'min_child_weight': 3.1896308745823183, 'num_rounds': 298.19866810959155, 'reg_alpha': 0.27065867995679893, 'reg_lambda': 1.3326635539401765, 'weight': 0.16342544218583369}\n",
      "100%|███████████████████████████████████████████████| 22/22 [02:32<00:00, 152.55s/trial, best loss: 247.54687877579468]\n",
      "22\n",
      "247.54687877579468\n",
      "{'learning_rate': 0.12277257256212021, 'max_delta_step': 0.33994039642580187, 'max_depth': 0, 'min_child_weight': 0.5065175007474233, 'num_rounds': 209.3694899343695, 'reg_alpha': 4.6760873578257875, 'reg_lambda': 4.484197931051557, 'weight': 0.13738315470495976}\n",
      "100%|███████████████████████████████████████████████| 23/23 [05:59<00:00, 359.52s/trial, best loss: 247.54687877579468]\n",
      "23\n",
      "247.54687877579468\n",
      "{'learning_rate': 0.12277257256212021, 'max_delta_step': 0.33994039642580187, 'max_depth': 0, 'min_child_weight': 0.5065175007474233, 'num_rounds': 209.3694899343695, 'reg_alpha': 4.6760873578257875, 'reg_lambda': 4.484197931051557, 'weight': 0.13738315470495976}\n",
      "100%|███████████████████████████████████████████████| 24/24 [02:13<00:00, 133.20s/trial, best loss: 247.54687877579468]\n",
      "24\n",
      "247.54687877579468\n",
      "{'learning_rate': 0.12277257256212021, 'max_delta_step': 0.33994039642580187, 'max_depth': 0, 'min_child_weight': 0.5065175007474233, 'num_rounds': 209.3694899343695, 'reg_alpha': 4.6760873578257875, 'reg_lambda': 4.484197931051557, 'weight': 0.13738315470495976}\n",
      "100%|███████████████████████████████████████████████| 25/25 [02:24<00:00, 144.32s/trial, best loss: 247.54687877579468]\n",
      "25\n",
      "247.54687877579468\n",
      "{'learning_rate': 0.12277257256212021, 'max_delta_step': 0.33994039642580187, 'max_depth': 0, 'min_child_weight': 0.5065175007474233, 'num_rounds': 209.3694899343695, 'reg_alpha': 4.6760873578257875, 'reg_lambda': 4.484197931051557, 'weight': 0.13738315470495976}\n",
      "100%|███████████████████████████████████████████████| 26/26 [04:24<00:00, 264.72s/trial, best loss: 247.54687877579468]\n",
      "26\n",
      "247.54687877579468\n",
      "{'learning_rate': 0.12277257256212021, 'max_delta_step': 0.33994039642580187, 'max_depth': 0, 'min_child_weight': 0.5065175007474233, 'num_rounds': 209.3694899343695, 'reg_alpha': 4.6760873578257875, 'reg_lambda': 4.484197931051557, 'weight': 0.13738315470495976}\n",
      "100%|███████████████████████████████████████████████| 27/27 [04:45<00:00, 285.29s/trial, best loss: 247.54687877579468]\n",
      "27\n",
      "247.54687877579468\n",
      "{'learning_rate': 0.12277257256212021, 'max_delta_step': 0.33994039642580187, 'max_depth': 0, 'min_child_weight': 0.5065175007474233, 'num_rounds': 209.3694899343695, 'reg_alpha': 4.6760873578257875, 'reg_lambda': 4.484197931051557, 'weight': 0.13738315470495976}\n",
      "100%|███████████████████████████████████████████████| 28/28 [03:10<00:00, 190.65s/trial, best loss: 247.54687877579468]\n",
      "28\n",
      "247.54687877579468\n",
      "{'learning_rate': 0.12277257256212021, 'max_delta_step': 0.33994039642580187, 'max_depth': 0, 'min_child_weight': 0.5065175007474233, 'num_rounds': 209.3694899343695, 'reg_alpha': 4.6760873578257875, 'reg_lambda': 4.484197931051557, 'weight': 0.13738315470495976}\n",
      "100%|███████████████████████████████████████████████| 29/29 [04:04<00:00, 244.47s/trial, best loss: 238.93838838049925]\n",
      "29\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 30/30 [03:02<00:00, 182.65s/trial, best loss: 238.93838838049925]\n",
      "30\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 31/31 [02:24<00:00, 144.28s/trial, best loss: 238.93838838049925]\n",
      "31\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 32/32 [01:46<00:00, 106.77s/trial, best loss: 238.93838838049925]\n",
      "32\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 33/33 [02:52<00:00, 172.44s/trial, best loss: 238.93838838049925]\n",
      "33\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 34/34 [02:31<00:00, 151.44s/trial, best loss: 238.93838838049925]\n",
      "34\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 35/35 [02:35<00:00, 155.68s/trial, best loss: 238.93838838049925]\n",
      "35\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|████████████████████████████████████████████████| 36/36 [00:57<00:00, 57.46s/trial, best loss: 238.93838838049925]\n",
      "36\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|████████████████████████████████████████████████| 37/37 [01:31<00:00, 91.08s/trial, best loss: 238.93838838049925]\n",
      "37\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 38/38 [02:37<00:00, 157.56s/trial, best loss: 238.93838838049925]\n",
      "38\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 39/39 [02:24<00:00, 144.43s/trial, best loss: 238.93838838049925]\n",
      "39\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|████████████████████████████████████████████████| 40/40 [00:53<00:00, 53.80s/trial, best loss: 238.93838838049925]\n",
      "40\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 41/41 [02:29<00:00, 149.24s/trial, best loss: 238.93838838049925]\n",
      "41\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 42/42 [02:37<00:00, 157.04s/trial, best loss: 238.93838838049925]\n",
      "42\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 43/43 [02:05<00:00, 125.71s/trial, best loss: 238.93838838049925]\n",
      "43\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|████████████████████████████████████████████████| 44/44 [01:21<00:00, 81.64s/trial, best loss: 238.93838838049925]\n",
      "44\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 45/45 [05:11<00:00, 311.42s/trial, best loss: 238.93838838049925]\n",
      "45\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 46/46 [03:15<00:00, 195.78s/trial, best loss: 238.93838838049925]\n",
      "46\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|████████████████████████████████████████████████| 47/47 [01:28<00:00, 88.73s/trial, best loss: 238.93838838049925]\n",
      "47\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 48/48 [03:09<00:00, 189.58s/trial, best loss: 238.93838838049925]\n",
      "48\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 49/49 [05:29<00:00, 329.50s/trial, best loss: 238.93838838049925]\n",
      "49\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 50/50 [04:15<00:00, 255.51s/trial, best loss: 238.93838838049925]\n",
      "50\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|████████████████████████████████████████████████| 51/51 [01:23<00:00, 83.99s/trial, best loss: 238.93838838049925]\n",
      "51\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|████████████████████████████████████████████████| 52/52 [01:34<00:00, 94.37s/trial, best loss: 238.93838838049925]\n",
      "52\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 53/53 [02:04<00:00, 124.23s/trial, best loss: 238.93838838049925]\n",
      "53\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 54/54 [02:16<00:00, 136.85s/trial, best loss: 238.93838838049925]\n",
      "54\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 55/55 [03:26<00:00, 206.19s/trial, best loss: 238.93838838049925]\n",
      "55\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 56/56 [01:49<00:00, 109.97s/trial, best loss: 238.93838838049925]\n",
      "56\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 57/57 [01:57<00:00, 117.15s/trial, best loss: 238.93838838049925]\n",
      "57\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|████████████████████████████████████████████████| 58/58 [01:18<00:00, 78.20s/trial, best loss: 238.93838838049925]\n",
      "58\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 59/59 [01:44<00:00, 104.82s/trial, best loss: 238.93838838049925]\n",
      "59\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 60/60 [02:38<00:00, 158.21s/trial, best loss: 238.93838838049925]\n",
      "60\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 61/61 [04:37<00:00, 277.09s/trial, best loss: 238.93838838049925]\n",
      "61\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 62/62 [01:47<00:00, 107.24s/trial, best loss: 238.93838838049925]\n",
      "62\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 63/63 [03:10<00:00, 190.45s/trial, best loss: 238.93838838049925]\n",
      "63\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 64/64 [02:45<00:00, 165.98s/trial, best loss: 238.93838838049925]\n",
      "64\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 65/65 [02:38<00:00, 158.17s/trial, best loss: 238.93838838049925]\n",
      "65\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 66/66 [02:50<00:00, 170.39s/trial, best loss: 238.93838838049925]\n",
      "66\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 67/67 [15:04<00:00, 904.12s/trial, best loss: 238.93838838049925]\n",
      "67\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 68/68 [01:49<00:00, 109.41s/trial, best loss: 238.93838838049925]\n",
      "68\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|████████████████████████████████████████████████| 69/69 [01:17<00:00, 77.09s/trial, best loss: 238.93838838049925]\n",
      "69\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 70/70 [05:29<00:00, 329.92s/trial, best loss: 238.93838838049925]\n",
      "70\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 71/71 [02:46<00:00, 166.59s/trial, best loss: 238.93838838049925]\n",
      "71\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 72/72 [03:12<00:00, 192.36s/trial, best loss: 238.93838838049925]\n",
      "72\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|████████████████████████████████████████████████| 73/73 [01:38<00:00, 98.28s/trial, best loss: 238.93838838049925]\n",
      "73\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|████████████████████████████████████████████████| 74/74 [01:35<00:00, 95.93s/trial, best loss: 238.93838838049925]\n",
      "74\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|████████████████████████████████████████████████| 75/75 [01:14<00:00, 74.71s/trial, best loss: 238.93838838049925]\n",
      "75\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 76/76 [05:12<00:00, 312.47s/trial, best loss: 238.93838838049925]\n",
      "76\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|████████████████████████████████████████████████| 77/77 [01:37<00:00, 97.14s/trial, best loss: 238.93838838049925]\n",
      "77\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 78/78 [02:06<00:00, 126.36s/trial, best loss: 238.93838838049925]\n",
      "78\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 79/79 [02:47<00:00, 167.80s/trial, best loss: 238.93838838049925]\n",
      "79\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 80/80 [02:34<00:00, 154.82s/trial, best loss: 238.93838838049925]\n",
      "80\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 81/81 [02:28<00:00, 148.78s/trial, best loss: 238.93838838049925]\n",
      "81\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 82/82 [02:11<00:00, 131.96s/trial, best loss: 238.93838838049925]\n",
      "82\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 83/83 [02:10<00:00, 130.76s/trial, best loss: 238.93838838049925]\n",
      "83\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 84/84 [02:45<00:00, 165.42s/trial, best loss: 238.93838838049925]\n",
      "84\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 85/85 [04:58<00:00, 298.96s/trial, best loss: 238.93838838049925]\n",
      "85\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 86/86 [04:56<00:00, 296.66s/trial, best loss: 238.93838838049925]\n",
      "86\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 87/87 [01:48<00:00, 108.91s/trial, best loss: 238.93838838049925]\n",
      "87\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 88/88 [02:05<00:00, 125.92s/trial, best loss: 238.93838838049925]\n",
      "88\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|████████████████████████████████████████████████| 89/89 [01:18<00:00, 78.06s/trial, best loss: 238.93838838049925]\n",
      "89\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 90/90 [02:13<00:00, 133.20s/trial, best loss: 238.93838838049925]\n",
      "90\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 91/91 [03:52<00:00, 232.80s/trial, best loss: 238.93838838049925]\n",
      "91\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 92/92 [02:30<00:00, 150.94s/trial, best loss: 238.93838838049925]\n",
      "92\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 93/93 [02:38<00:00, 158.71s/trial, best loss: 238.93838838049925]\n",
      "93\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 94/94 [05:43<00:00, 343.66s/trial, best loss: 238.93838838049925]\n",
      "94\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 95/95 [02:50<00:00, 170.81s/trial, best loss: 238.93838838049925]\n",
      "95\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 96/96 [01:49<00:00, 109.07s/trial, best loss: 238.93838838049925]\n",
      "96\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 97/97 [02:10<00:00, 130.12s/trial, best loss: 238.93838838049925]\n",
      "97\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      "100%|███████████████████████████████████████████████| 98/98 [07:23<00:00, 444.00s/trial, best loss: 238.93838838049925]\n",
      "98\n",
      "238.93838838049925\n",
      "{'learning_rate': 0.23258374432866033, 'max_delta_step': 4.087581210801292, 'max_depth': 4, 'min_child_weight': 1.925593494006774, 'num_rounds': 335.9808545767529, 'reg_alpha': 1.423400568682085, 'reg_lambda': 3.955876924272332, 'weight': 0.17114897417853941}\n",
      " 99%|█████████████████████████████████████████████████████████████████████████▎| 98/99 [01:38<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-2880c97698ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     best = fmin(fn = cross_validation_neg_acc_gradient_boosting, space = space_gradient_boosting,\n\u001b[1;32m----> 5\u001b[1;33m                 algo = rand.suggest, max_evals = i, trials = trials)\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m#np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"results\", \"cross_validation_binding_ESM1b.npy\"), trials.best_trial)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"results\", \"cross_validation_binding_ESM1b_argmin.npy\"), trials.argmin)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Predicting_Km\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    520\u001b[0m             \u001b[0mshow_progressbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m             \u001b[0mearly_stop_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stop_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m             \u001b[0mtrials_save_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrials_save_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    523\u001b[0m         )\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Predicting_Km\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    697\u001b[0m             \u001b[0mshow_progressbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[0mearly_stop_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stop_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             \u001b[0mtrials_save_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrials_save_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         )\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Predicting_Km\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;31m# next line is where the fmin is actually executed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Predicting_Km\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Predicting_Km\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m                     \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Predicting_Km\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"job exception: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Predicting_Km\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    905\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m             )\n\u001b[1;32m--> 907\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-6c36b8ef6986>\u001b[0m in \u001b[0;36mcross_validation_neg_acc_gradient_boosting\u001b[1;34m(param)\u001b[0m\n\u001b[0;32m     15\u001b[0m                          label = np.array(train_y[train_index]))\n\u001b[0;32m     16\u001b[0m         \u001b[0mdvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mbst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_round\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0my_valid_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mvalidation_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Predicting_Km\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    210\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Predicting_Km\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Predicting_Km\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1159\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0;32m   1160\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1161\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1162\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "\n",
    "for i in range(1,2000):\n",
    "    best = fmin(fn = cross_validation_neg_acc_gradient_boosting, space = space_gradient_boosting,\n",
    "                algo = rand.suggest, max_evals = i, trials = trials)\n",
    "    #np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"results\", \"cross_validation_binding_ESM1b.npy\"), trials.best_trial)\n",
    "    #np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"results\", \"cross_validation_binding_ESM1b_argmin.npy\"), trials.argmin)\n",
    "    print(i)\n",
    "    print(trials.best_trial[\"result\"][\"loss\"])\n",
    "    print(trials.argmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.23258374432866033,\n",
       " 'max_delta_step': 4.087581210801292,\n",
       " 'max_depth': 4,\n",
       " 'min_child_weight': 1.925593494006774,\n",
       " 'num_rounds': 335.9808545767529,\n",
       " 'reg_alpha': 1.423400568682085,\n",
       " 'reg_lambda': 3.955876924272332,\n",
       " 'weight': 0.17114897417853941}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.argmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best set of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.15413360344307642,\n",
    "         'max_delta_step': 1.2762890808461014,\n",
    "         'max_depth': 11,\n",
    "         'min_child_weight': 1.702130735213737,\n",
    "         'num_rounds': 398.33364651483566,\n",
    "         'reg_alpha': 2.5568884181958356,\n",
    "         'reg_lambda': 2.5526015056842817,\n",
    "         'weight': 0.13909428322475764}\n",
    "\n",
    "param = {'learning_rate': 0.12771337495138718,\n",
    "         'max_delta_step': 3.080851382419611,\n",
    "         'max_depth': 13,\n",
    "         'min_child_weight': 2.68947814956559,\n",
    "         'num_rounds': 332.92969059815346,\n",
    "         'reg_alpha': 1.4293630231664674,\n",
    "         'reg_lambda': 0.12220981612600046,\n",
    "         'weight': 0.11412319177763543}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.23258374432866033,\n",
    " 'max_delta_step': 4.087581210801292,\n",
    " 'max_depth': 13,\n",
    " 'min_child_weight': 1.925593494006774,\n",
    " 'num_rounds': 335.9808545767529,\n",
    " 'reg_alpha': 1.423400568682085,\n",
    " 'reg_lambda': 3.955876924272332,\n",
    " 'weight': 0.17114897417853941}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Repeating 5-fold CV for best set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss values: [239.6373774252441, 205.63435294814062, 253.13824236782585, 255.40983459530463, 240.87213456598107]\n",
      "Accuracies: [0.847457627118644, 0.8537383177570094, 0.8486901535682023, 0.8453406421299922, 0.8345679012345679]\n",
      "ROC-AUC scores: [0.8865283513815269, 0.9093155599507349, 0.9002365323664021, 0.8896475880752803, 0.8946331655005557]\n"
     ]
    }
   ],
   "source": [
    "num_round = param[\"num_rounds\"]\n",
    "param[\"tree_method\"] = \"gpu_hist\"\n",
    "param[\"sampling_method\"] = \"gradient_based\"\n",
    "param['objective'] = 'binary:logistic'\n",
    "weights = np.array([param[\"weight\"] if binding == 0 else 1.0 for binding in df_train[\"Binding\"]])\n",
    "\n",
    "del param[\"num_rounds\"]\n",
    "del param[\"weight\"]\n",
    "\n",
    "\n",
    "loss = []\n",
    "accuracy = []\n",
    "ROC_AUC = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(np.array(train_X[train_index]), weight = weights[train_index],\n",
    "                     label = np.array(train_y[train_index]))\n",
    "    dvalid = xgb.DMatrix(np.array(train_X[test_index]))\n",
    "    bst = xgb.train(param,  dtrain, int(num_round), verbose_eval=1)\n",
    "    y_valid_pred = np.round(bst.predict(dvalid))\n",
    "    validation_y = train_y[test_index]\n",
    "\n",
    "    #calculate loss:\n",
    "    false_positive = 100*(1-np.mean(np.array(validation_y)[y_valid_pred == 1]))\n",
    "    false_negative = 100*(np.mean(np.array(validation_y)[y_valid_pred == 0]))\n",
    "    logging.info(\"False positive rate: \" + str(false_positive)+ \"; False negative rate: \" + str(false_negative))\n",
    "    loss.append(2*(false_negative**2) + false_positive**1.3)\n",
    "    #calculate accuracy:\n",
    "    accuracy.append(np.mean(y_valid_pred == np.array(validation_y)))\n",
    "    #calculate ROC-AUC score:\n",
    "    ROC_AUC.append(roc_auc_score(np.array(validation_y), bst.predict(dvalid)))\n",
    "    \n",
    "print(\"Loss values: %s\" %loss) \n",
    "print(\"Accuracies: %s\" %accuracy)\n",
    "print(\"ROC-AUC scores: %s\" %ROC_AUC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) 3. Training and validating the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model and validating it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.8455397431447415, ROC-AUC score for test set: 0.90368561948571, MCC: 0.612486423183225\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(np.array(train_X), weight = weights, label = np.array(train_y),\n",
    "                feature_names= feature_names)\n",
    "dtest = xgb.DMatrix(np.array(test_X), label = np.array(test_y),\n",
    "                    feature_names= feature_names)\n",
    "\n",
    "bst = xgb.train(param,  dtrain, int(num_round), verbose_eval=1)\n",
    "y_test_pred = np.round(bst.predict(dtest))\n",
    "acc_test = np.mean(y_test_pred == np.array(test_y))\n",
    "roc_auc = roc_auc_score(np.array(test_y), bst.predict(dtest))\n",
    "mcc = matthews_corrcoef(np.array(test_y), y_test_pred)\n",
    "\n",
    "print(\"Accuracy on test set: %s, ROC-AUC score for test set: %s, MCC: %s\"  % (acc_test, roc_auc, mcc))\n",
    "\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b_ECFP.npy\"), bst.predict(dtest))\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"y_test_true_xgboost_ESM1b_ECFP.npy\"), test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) ESM1b and GNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Creating numpy arrays with input vectors and output variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_and_output_data(df):\n",
    "    X = ();\n",
    "    y = ();\n",
    "        \n",
    "    for ind in df.index:\n",
    "        emb = df[\"ESM1b\"][ind]\n",
    "        ecfp = df[\"GNN rep\"][ind]\n",
    "                \n",
    "        X = X +(np.concatenate([ecfp, emb]), );\n",
    "        y = y + (df[\"Binding\"][ind], );\n",
    "\n",
    "    return(X,y)\n",
    "\n",
    "train_X, train_y =  create_input_and_output_data(df = df_train)\n",
    "test_X, test_y =  create_input_and_output_data(df = df_test)\n",
    "\n",
    "\n",
    "feature_names =  [\"GNN rep_\" + str(i) for i in range(50)]\n",
    "feature_names = feature_names + [\"ESM1b_\" + str(i) for i in range(1280)]\n",
    "\n",
    "train_X = np.array(train_X)\n",
    "test_X  = np.array(test_X)\n",
    "\n",
    "train_y = np.array(train_y)\n",
    "test_y  = np.array(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Performing hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3812/3269672333.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m#Defining search space for hyperparameter optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m space_gradient_boosting = {\"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.5),\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;34m\"max_depth\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_depth\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;34m\"reg_lambda\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"reg_lambda\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hp' is not defined"
     ]
    }
   ],
   "source": [
    "def cross_validation_neg_acc_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    weights = np.array([param[\"weight\"] if binding == 0 else 1.0 for binding in df_train[\"Binding\"]])\n",
    "    \n",
    "    del param[\"num_rounds\"]\n",
    "    del param[\"weight\"]\n",
    "    \n",
    "    loss = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(np.array(train_X[train_index]), weight = weights[train_index],\n",
    "                         label = np.array(train_y[train_index]))\n",
    "        dvalid = xgb.DMatrix(np.array(train_X[test_index]))\n",
    "        bst = xgb.train(param,  dtrain, int(num_round), verbose_eval=1)\n",
    "        y_valid_pred = np.round(bst.predict(dvalid))\n",
    "        validation_y = train_y[test_index]\n",
    "    \n",
    "        false_positive = 100*(1-np.mean(np.array(validation_y)[y_valid_pred == 1]))\n",
    "        false_negative = 100*(np.mean(np.array(validation_y)[y_valid_pred == 0]))\n",
    "        logging.info(\"False positive rate: \" + str(false_positive)+ \"; False negative rate: \" + str(false_negative))\n",
    "        loss.append(2*(false_negative**2) + false_positive**1.3)\n",
    "    return(np.mean(loss))\n",
    "\n",
    "#Defining search space for hyperparameter optimization\n",
    "space_gradient_boosting = {\"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.5),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", [9,10,11,12,13]),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 200, 400),\n",
    "    \"weight\" : hp.uniform(\"weight\", 0.1,0.33)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing a random grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''trials = Trials()\n",
    "\n",
    "for i in range(1,2000):\n",
    "    best = fmin(fn = cross_validation_neg_acc_gradient_boosting, space = space_gradient_boosting,\n",
    "                algo = rand.suggest, max_evals = i, trials = trials)\n",
    "    np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"results\", \"cross_validation_binding_ESM1b.npy\"), trials.best_trial)\n",
    "    np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"results\", \"cross_validation_binding_ESM1b_argmin.npy\"), trials.argmin)\n",
    "    logging.info(i)\n",
    "    logging.info(trials.best_trial[\"result\"][\"loss\"])\n",
    "    logging.info(trials.argmin)''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best set of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.14400235501414152,\n",
    "         'max_delta_step': 0.814795683926195,\n",
    "         'max_depth': 11,\n",
    "         'min_child_weight': 4.985914250393847,\n",
    "         'num_rounds': 326.61065381972924,\n",
    "         'reg_alpha': 2.3376079081750634,\n",
    "         'reg_lambda': 3.8256939813631936,\n",
    "         'weight': 0.10867442666803548}\n",
    "\n",
    "param = {'learning_rate': 0.18650490181254992,\n",
    "         'max_delta_step': 3.747845574621026,\n",
    "         'max_depth': 10,\n",
    "         'min_child_weight': 0.3985828341503377,\n",
    "         'num_rounds': 366.6289439088624,\n",
    "         'reg_alpha': 0.8924081775198611,\n",
    "         'reg_lambda': 4.888409483879253, \n",
    "         'weight': 0.14249550342115477}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Repeating 5-fold CV for best set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = param[\"num_rounds\"]\n",
    "param[\"tree_method\"] = \"gpu_hist\"\n",
    "param[\"sampling_method\"] = \"gradient_based\"\n",
    "param['objective'] = 'binary:logistic'\n",
    "weights = np.array([param[\"weight\"] if binding == 0 else 1.0 for binding in df_train[\"Binding\"]])\n",
    "\n",
    "del param[\"num_rounds\"]\n",
    "del param[\"weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:13:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:15:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:17:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:20:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:52:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Loss values: [127.93542431344466, 122.79501284863223, 132.211203999234, 126.27687643490749, 131.02385081797956]\n",
      "Accuracies: [0.9080177971488241, 0.9076071922544952, 0.906319241336524, 0.9075287865367582, 0.9075296031817771]\n",
      "ROC-AUC scores: [0.9481174271320283, 0.9501233965053054, 0.9437746666563114, 0.9507466012024709, 0.9510292058347031]\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "accuracy = []\n",
    "ROC_AUC = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(np.array(train_X[train_index]), weight = weights[train_index],\n",
    "                     label = np.array(train_y[train_index]))\n",
    "    dvalid = xgb.DMatrix(np.array(train_X[test_index]))\n",
    "    bst = xgb.train(param,  dtrain, int(num_round), verbose_eval=1)\n",
    "    y_valid_pred = np.round(bst.predict(dvalid))\n",
    "    validation_y = train_y[test_index]\n",
    "\n",
    "    #calculate loss:\n",
    "    false_positive = 100*(1-np.mean(np.array(validation_y)[y_valid_pred == 1]))\n",
    "    false_negative = 100*(np.mean(np.array(validation_y)[y_valid_pred == 0]))\n",
    "    logging.info(\"False positive rate: \" + str(false_positive)+ \"; False negative rate: \" + str(false_negative))\n",
    "    loss.append(2*(false_negative**2) + false_positive**1.3)\n",
    "    #calculate accuracy:\n",
    "    accuracy.append(np.mean(y_valid_pred == np.array(validation_y)))\n",
    "    #calculate ROC-AUC score:\n",
    "    ROC_AUC.append(roc_auc_score(np.array(validation_y), bst.predict(dvalid)))\n",
    "    \n",
    "print(\"Loss values: %s\" %loss) \n",
    "print(\"Accuracies: %s\" %accuracy)\n",
    "print(\"ROC-AUC scores: %s\" %ROC_AUC)\n",
    "\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"acc_CV_xgboost_ESM1b_GNN.npy\"), np.array(accuracy))\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"loss_CV_xgboost_ESM1b_GNN.npy\"), np.array(loss))\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"ROC_AUC_CV_xgboost_ESM1b_GNN.npy\"), np.array(ROC_AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) 3. Training and validating the final model\n",
    "Training the model and validating it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:54:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy on test set: 0.9051646706586827, ROC-AUC score for test set: 0.9455840254566843, MCC: 0.7509981408842288\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(np.array(train_X), weight = weights, label = np.array(train_y),\n",
    "                feature_names= feature_names)\n",
    "dtest = xgb.DMatrix(np.array(test_X), label = np.array(test_y),\n",
    "                    feature_names= feature_names)\n",
    "\n",
    "bst = xgb.train(param,  dtrain, int(num_round), verbose_eval=1)\n",
    "y_test_pred = np.round(bst.predict(dtest))\n",
    "acc_test = np.mean(y_test_pred == np.array(test_y))\n",
    "roc_auc = roc_auc_score(np.array(test_y), bst.predict(dtest))\n",
    "mcc = matthews_corrcoef(np.array(test_y), y_test_pred)\n",
    "\n",
    "print(\"Accuracy on test set: %s, ROC-AUC score for test set: %s, MCC: %s\"  % (acc_test, roc_auc, mcc))\n",
    "\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b_GNN.npy\"), bst.predict(dtest))\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"y_test_true_xgboost_ESM1b_GNN.npy\"), test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) ESM1b_ts and ECFP:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Creating numpy arrays with input vectors and output variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_and_output_data(df):\n",
    "    X = ();\n",
    "    y = ();\n",
    "    \n",
    "    for ind in df.index:\n",
    "        emb = df[\"ESM1b_ts\"][ind]\n",
    "        ecfp = np.array(list(df[\"ECFP\"][ind])).astype(int)\n",
    "                \n",
    "        X = X +(np.concatenate([ecfp, emb]), );\n",
    "        y = y + (df[\"Binding\"][ind], );\n",
    "\n",
    "    return(X,y)\n",
    "\n",
    "train_X, train_y =  create_input_and_output_data(df = df_train)\n",
    "test_X, test_y =  create_input_and_output_data(df = df_test)\n",
    "\n",
    "\n",
    "feature_names =  [\"ECFP_\" + str(i) for i in range(1024)]\n",
    "feature_names = feature_names + [\"ESM1b_ts_\" + str(i) for i in range(1280)]\n",
    "\n",
    "train_X = np.array(train_X)\n",
    "test_X  = np.array(test_X)\n",
    "\n",
    "train_y = np.array(train_y)\n",
    "test_y  = np.array(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Performing hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_neg_acc_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    weights = np.array([param[\"weight\"] if binding == 0 else 1.0 for binding in df_train[\"Binding\"]])\n",
    "    \n",
    "    del param[\"num_rounds\"]\n",
    "    del param[\"weight\"]\n",
    "    \n",
    "    loss = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(np.array(train_X[train_index]), weight = weights[train_index],\n",
    "                         label = np.array(train_y[train_index]))\n",
    "        dvalid = xgb.DMatrix(np.array(train_X[test_index]))\n",
    "        bst = xgb.train(param,  dtrain, int(num_round), verbose_eval=1)\n",
    "        y_valid_pred = np.round(bst.predict(dvalid))\n",
    "        validation_y = train_y[test_index]\n",
    "    \n",
    "        false_positive = 100*(1-np.mean(np.array(validation_y)[y_valid_pred == 1]))\n",
    "        false_negative = 100*(np.mean(np.array(validation_y)[y_valid_pred == 0]))\n",
    "        logging.info(\"False positive rate: \" + str(false_positive)+ \"; False negative rate: \" + str(false_negative))\n",
    "        loss.append(2*(false_negative**2) + false_positive**1.3)\n",
    "    return(np.mean(loss))\n",
    "\n",
    "#Defining search space for hyperparameter optimization\n",
    "space_gradient_boosting = {\"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.5),\n",
    "                            \"max_depth\": hp.choice(\"max_depth\", [9,10,11,12,13]),\n",
    "                            \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "                            \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "                            \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "                            \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "                            \"num_rounds\":  hp.uniform(\"num_rounds\", 200, 400),\n",
    "                            \"weight\" : hp.uniform(\"weight\", 0.1,0.33)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing a random grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''trials = Trials()\n",
    "\n",
    "for i in range(1,2000):\n",
    "    best = fmin(fn = cross_validation_neg_acc_gradient_boosting, space = space_gradient_boosting,\n",
    "                algo = rand.suggest, max_evals = i, trials = trials)\n",
    "    np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"results\", \"cross_validation_binding_ESM1b.npy\"), trials.best_trial)\n",
    "    np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"results\", \"cross_validation_binding_ESM1b_argmin.npy\"), trials.argmin)\n",
    "    logging.info(i)\n",
    "    logging.info(trials.best_trial[\"result\"][\"loss\"])\n",
    "    logging.info(trials.argmin)''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best set of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.18394760595697052,\n",
    "         'max_delta_step': 2.100681119544991,\n",
    "         'max_depth': 11,\n",
    "         'min_child_weight': 0.7270445069860024,\n",
    "         'num_rounds': 325.1827926302196,\n",
    "         'reg_alpha': 0.17887744610095235,\n",
    "         'reg_lambda': 1.4959549363997264,\n",
    "         'weight': 0.11976484745202556}\n",
    "\n",
    "param = {'learning_rate': 0.31553117247348733,\n",
    "         'max_delta_step': 1.7726044219753656,\n",
    "         'max_depth': 10,\n",
    "         'min_child_weight': 1.3845040588450772,\n",
    "         'num_rounds': 342.68325188584106,\n",
    "         'reg_alpha': 0.531395259755843,\n",
    "         'reg_lambda': 3.744980563764689,\n",
    "         'weight': 0.26187490421514203}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"tree_method\"] = \"gpu_hist\"\n",
    "param[\"sampling_method\"] = \"gradient_based\"\n",
    "param['objective'] = 'binary:logistic'\n",
    "weights = np.array([param[\"weight\"] if binding == 0 else 1.0 for binding in df_train[\"Binding\"]])\n",
    "\n",
    "del param[\"num_rounds\"]\n",
    "del param[\"weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Repeating 5-fold CV for best set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:57:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:00:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:02:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:04:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:06:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Loss values: [118.64570140259957, 109.55544934268875, 136.58107225995332, 112.14992450710591, 116.58008809267714]\n",
      "Accuracies: [0.9095614274039772, 0.9113877362840018, 0.8988744128334663, 0.9105403011514615, 0.9075296031817771]\n",
      "ROC-AUC scores: [0.9531463757576579, 0.9543092505120799, 0.9443616020172131, 0.9560860893869755, 0.9515226897235054]\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "accuracy = []\n",
    "ROC_AUC = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(np.array(train_X[train_index]), weight = weights[train_index],\n",
    "                     label = np.array(train_y[train_index]))\n",
    "    dvalid = xgb.DMatrix(np.array(train_X[test_index]))\n",
    "    bst = xgb.train(param,  dtrain, int(num_round), verbose_eval=1)\n",
    "    y_valid_pred = np.round(bst.predict(dvalid))\n",
    "    validation_y = train_y[test_index]\n",
    "\n",
    "    #calculate loss:\n",
    "    false_positive = 100*(1-np.mean(np.array(validation_y)[y_valid_pred == 1]))\n",
    "    false_negative = 100*(np.mean(np.array(validation_y)[y_valid_pred == 0]))\n",
    "    logging.info(\"False positive rate: \" + str(false_positive)+ \"; False negative rate: \" + str(false_negative))\n",
    "    loss.append(2*(false_negative**2) + false_positive**1.3)\n",
    "    #calculate accuracy:\n",
    "    accuracy.append(np.mean(y_valid_pred == np.array(validation_y)))\n",
    "    #calculate ROC-AUC score:\n",
    "    ROC_AUC.append(roc_auc_score(np.array(validation_y), bst.predict(dvalid)))\n",
    "    \n",
    "print(\"Loss values: %s\" %loss) \n",
    "print(\"Accuracies: %s\" %accuracy)\n",
    "print(\"ROC-AUC scores: %s\" %ROC_AUC)\n",
    "\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"acc_CV_xgboost_ESM1b_ts_ECFP.npy\"), np.array(accuracy))\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"loss_CV_xgboost_ESM1b_ts_ECFP.npy\"), np.array(loss))\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"ROC_AUC_CV_xgboost_ESM1b_ts_ECFP.npy\"), np.array(ROC_AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) 3. Training and validating the final model\n",
    "Training the model and validating it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:09:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy on test set: 0.9063622754491018, ROC-AUC score for test set: 0.9499652082794869, MCC: 0.7594154904873446\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(np.array(train_X), weight = weights, label = np.array(train_y),\n",
    "                feature_names= feature_names)\n",
    "dtest = xgb.DMatrix(np.array(test_X), label = np.array(test_y),\n",
    "                    feature_names= feature_names)\n",
    "\n",
    "bst = xgb.train(param,  dtrain, int(num_round), verbose_eval=1)\n",
    "y_test_pred = np.round(bst.predict(dtest))\n",
    "acc_test = np.mean(y_test_pred == np.array(test_y))\n",
    "roc_auc = roc_auc_score(np.array(test_y), bst.predict(dtest))\n",
    "mcc = matthews_corrcoef(np.array(test_y), y_test_pred)\n",
    "\n",
    "print(\"Accuracy on test set: %s, ROC-AUC score for test set: %s, MCC: %s\"  % (acc_test, roc_auc, mcc))\n",
    "\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b_ts_ECFP.npy\"), bst.predict(dtest))\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"y_test_true_xgboost_ESM1b_ts_ECFP.npy\"), test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) ESM1b_ts and GNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Creating numpy arrays with input vectors and output variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_and_output_data(df):\n",
    "    X = ();\n",
    "    y = ();\n",
    "    \n",
    "    for ind in df.index:\n",
    "        emb = df[\"ESM1b_ts\"][ind]\n",
    "        ecfp = df[\"GNN rep\"][ind]\n",
    "                \n",
    "        X = X +(np.concatenate([ecfp, emb]), );\n",
    "        y = y + (df[\"Binding\"][ind], );\n",
    "\n",
    "    return(X,y)\n",
    "\n",
    "train_X, train_y =  create_input_and_output_data(df = df_train)\n",
    "test_X, test_y =  create_input_and_output_data(df = df_test)\n",
    "\n",
    "\n",
    "feature_names =  [\"GNN rep_\" + str(i) for i in range(50)]\n",
    "feature_names = feature_names + [\"ESM1b_\" + str(i) for i in range(1280)]\n",
    "\n",
    "train_X = np.array(train_X)\n",
    "test_X  = np.array(test_X)\n",
    "\n",
    "train_y = np.array(train_y)\n",
    "test_y  = np.array(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Performing hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7960/3269672333.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m#Defining search space for hyperparameter optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m space_gradient_boosting = {\"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.5),\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;34m\"max_depth\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_depth\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;34m\"reg_lambda\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"reg_lambda\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hp' is not defined"
     ]
    }
   ],
   "source": [
    "def cross_validation_neg_acc_gradient_boosting(param):\n",
    "    num_round = param[\"num_rounds\"]\n",
    "    param[\"tree_method\"] = \"gpu_hist\"\n",
    "    param[\"sampling_method\"] = \"gradient_based\"\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    weights = np.array([param[\"weight\"] if binding == 0 else 1.0 for binding in df_train[\"Binding\"]])\n",
    "    \n",
    "    del param[\"num_rounds\"]\n",
    "    del param[\"weight\"]\n",
    "    \n",
    "    loss = []\n",
    "    for i in range(5):\n",
    "        train_index, test_index  = train_indices[i], test_indices[i]\n",
    "        dtrain = xgb.DMatrix(np.array(train_X[train_index]), weight = weights[train_index],\n",
    "                         label = np.array(train_y[train_index]))\n",
    "        dvalid = xgb.DMatrix(np.array(train_X[test_index]))\n",
    "        bst = xgb.train(param,  dtrain, int(num_round), verbose_eval=1)\n",
    "        y_valid_pred = np.round(bst.predict(dvalid))\n",
    "        validation_y = train_y[test_index]\n",
    "    \n",
    "        false_positive = 100*(1-np.mean(np.array(validation_y)[y_valid_pred == 1]))\n",
    "        false_negative = 100*(np.mean(np.array(validation_y)[y_valid_pred == 0]))\n",
    "        logging.info(\"False positive rate: \" + str(false_positive)+ \"; False negative rate: \" + str(false_negative))\n",
    "        loss.append(2*(false_negative**2) + false_positive**1.3)\n",
    "    return(np.mean(loss))\n",
    "\n",
    "#Defining search space for hyperparameter optimization\n",
    "space_gradient_boosting = {\"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.5),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", [9,10,11,12,13]),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 5),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"max_delta_step\": hp.uniform(\"max_delta_step\", 0, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.1, 15),\n",
    "    \"num_rounds\":  hp.uniform(\"num_rounds\", 200, 400),\n",
    "    \"weight\" : hp.uniform(\"weight\", 0.1,0.33)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing a random grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''trials = Trials()\n",
    "\n",
    "for i in range(1,2000):\n",
    "    best = fmin(fn = cross_validation_neg_acc_gradient_boosting, space = space_gradient_boosting,\n",
    "                algo = rand.suggest, max_evals = i, trials = trials)\n",
    "    np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"results\", \"cross_validation_binding_ESM1b.npy\"), trials.best_trial)\n",
    "    np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"results\", \"cross_validation_binding_ESM1b_argmin.npy\"), trials.argmin)\n",
    "    logging.info(i)\n",
    "    logging.info(trials.best_trial[\"result\"][\"loss\"])\n",
    "    logging.info(trials.argmin)''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best set of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'learning_rate': 0.2450500663744065,\n",
    "         'max_delta_step': 2.382647656857187,\n",
    "         'max_depth': 11,\n",
    "         'min_child_weight': 1.222014993565574, \n",
    "         'num_rounds': 379.3863424395678,\n",
    "         'reg_alpha': 1.7242896864948025,\n",
    "         'reg_lambda': 2.845463948389928,\n",
    "         'weight': 0.10896532373464474}\n",
    "\n",
    "param = {'learning_rate': 0.18444025726334898,\n",
    "         'max_delta_step': 3.2748796106084117,\n",
    "         'max_depth': 13,\n",
    "         'min_child_weight': 3.1946753845027738,\n",
    "         'num_rounds': 314.1036429221291,\n",
    "         'reg_alpha': 0.48821021807600673,\n",
    "         'reg_lambda': 2.6236829011598073,\n",
    "         'weight': 0.1264521266931227}\n",
    "\n",
    "num_round = param[\"num_rounds\"]\n",
    "param[\"tree_method\"] = \"gpu_hist\"\n",
    "param[\"sampling_method\"] = \"gradient_based\"\n",
    "param['objective'] = 'binary:logistic'\n",
    "weights = np.array([param[\"weight\"] if binding == 0 else 1.0 for binding in df_train[\"Binding\"]])\n",
    "\n",
    "del param[\"num_rounds\"]\n",
    "del param[\"weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Repeating 5-fold CV for best set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:11:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:13:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:14:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:15:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:16:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Loss values: [107.10779217320857, 97.97889476323061, 117.6449558838506, 92.98319474363471, 102.69988687285533]\n",
      "Accuracies: [0.9046581312993734, 0.9083448593822038, 0.90064699104848, 0.9066430469441984, 0.9074392117870379]\n",
      "ROC-AUC scores: [0.9584373172944172, 0.9558062039027504, 0.9500524479937247, 0.9617684224032785, 0.9569448004040885]\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "accuracy = []\n",
    "ROC_AUC = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_index, test_index  = train_indices[i], test_indices[i]\n",
    "    dtrain = xgb.DMatrix(np.array(train_X[train_index]), weight = weights[train_index],\n",
    "                     label = np.array(train_y[train_index]))\n",
    "    dvalid = xgb.DMatrix(np.array(train_X[test_index]))\n",
    "    bst = xgb.train(param,  dtrain, int(num_round), verbose_eval=1)\n",
    "    y_valid_pred = np.round(bst.predict(dvalid))\n",
    "    validation_y = train_y[test_index]\n",
    "\n",
    "    #calculate loss:\n",
    "    false_positive = 100*(1-np.mean(np.array(validation_y)[y_valid_pred == 1]))\n",
    "    false_negative = 100*(np.mean(np.array(validation_y)[y_valid_pred == 0]))\n",
    "    logging.info(\"False positive rate: \" + str(false_positive)+ \"; False negative rate: \" + str(false_negative))\n",
    "    loss.append(2*(false_negative**2) + false_positive**1.3)\n",
    "    #calculate accuracy:\n",
    "    accuracy.append(np.mean(y_valid_pred == np.array(validation_y)))\n",
    "    #calculate ROC-AUC score:\n",
    "    ROC_AUC.append(roc_auc_score(np.array(validation_y), bst.predict(dvalid)))\n",
    "    \n",
    "print(\"Loss values: %s\" %loss) \n",
    "print(\"Accuracies: %s\" %accuracy)\n",
    "print(\"ROC-AUC scores: %s\" %ROC_AUC)\n",
    "\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"acc_CV_xgboost_ESM1b_ts_GNN.npy\"), np.array(accuracy))\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"loss_CV_xgboost_ESM1b_ts_GNN.npy\"), np.array(loss))\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"ROC_AUC_CV_xgboost_ESM1b_ts_GNN.npy\"), np.array(ROC_AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) 3. Training and validating the final model\n",
    "Training the model and validating it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:18:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy on test set: 0.9028443113772455, ROC-AUC score for test set: 0.9540429057244825, MCC: 0.7544314356676812\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(np.array(train_X), weight = weights, label = np.array(train_y),\n",
    "                feature_names= feature_names)\n",
    "dtest = xgb.DMatrix(np.array(test_X), label = np.array(test_y),\n",
    "                    feature_names= feature_names)\n",
    "\n",
    "bst = xgb.train(param,  dtrain, int(num_round), verbose_eval=1)\n",
    "y_test_pred = np.round(bst.predict(dtest))\n",
    "acc_test = np.mean(y_test_pred == np.array(test_y))\n",
    "roc_auc = roc_auc_score(np.array(test_y), bst.predict(dtest))\n",
    "mcc = matthews_corrcoef(np.array(test_y), y_test_pred)\n",
    "\n",
    "print(\"Accuracy on test set: %s, ROC-AUC score for test set: %s, MCC: %s\"  % (acc_test, roc_auc, mcc))\n",
    "\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"y_test_pred_xgboost_ESM1b_ts_GNN.npy\"), bst.predict(dtest))\n",
    "np.save(join(CURRENT_DIR, \"..\" ,\"data\", \"training_results\", \"y_test_true_xgboost_ESM1b_ts_GNN.npy\"), test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
